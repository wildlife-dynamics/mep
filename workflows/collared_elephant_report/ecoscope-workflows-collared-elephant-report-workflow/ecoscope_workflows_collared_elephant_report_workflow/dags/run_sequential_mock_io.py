# AUTOGENERATED BY ECOSCOPE-WORKFLOWS; see fingerprint in README.md for details

# ruff: noqa: E402

"""WARNING: This file is generated in a testing context and should not be used in production.
Lines specific to the testing context are marked with a test tube emoji (ðŸ§ª) to indicate
that they would not be included (or would be different) in the production version of this file.
"""

import json
import os
import warnings  # ðŸ§ª
from ecoscope_workflows_core.testing import create_task_magicmock  # ðŸ§ª


from ecoscope_workflows_core.tasks.config import set_workflow_details
from ecoscope_workflows_core.tasks.io import set_gee_connection
from ecoscope_workflows_core.tasks.io import set_er_connection
from ecoscope_workflows_ext_ecoscope.tasks.results import set_base_maps
from ecoscope_workflows_core.tasks.groupby import set_groupers
from ecoscope_workflows_core.tasks.filter import set_time_range
from ecoscope_workflows_ext_mep.tasks import create_directory
from ecoscope_workflows_ext_mep.tasks import download_land_dx
from ecoscope_workflows_ext_mep.tasks import download_file_and_persist
from ecoscope_workflows_ext_mep.tasks import load_landdx_aoi
from ecoscope_workflows_ext_mep.tasks import split_gdf_by_column
from ecoscope_workflows_ext_mep.tasks import annotate_gdf_dict_with_geometry_type
from ecoscope_workflows_ext_mep.tasks import create_map_layers_from_annotated_dict
from ecoscope_workflows_ext_mep.tasks import get_subjects_info
from ecoscope_workflows_ext_ecoscope.tasks.transformation import normalize_column
from ecoscope_workflows_core.tasks.transformation import map_columns

get_subjectgroup_observations = create_task_magicmock(  # ðŸ§ª
    anchor="ecoscope_workflows_ext_ecoscope.tasks.io",  # ðŸ§ª
    func_name="get_subjectgroup_observations",  # ðŸ§ª
)  # ðŸ§ª
from ecoscope_workflows_ext_ecoscope.tasks.preprocessing import process_relocations
from ecoscope_workflows_ext_mep.tasks import compute_maturity
from ecoscope_workflows_core.tasks.groupby import split_groups
from ecoscope_workflows_ext_mep.tasks import download_profile_photo
from ecoscope_workflows_ext_mep.tasks import persist_subject_info

get_events = create_task_magicmock(  # ðŸ§ª
    anchor="ecoscope_workflows_ext_ecoscope.tasks.io",  # ðŸ§ª
    func_name="get_events",  # ðŸ§ª
)  # ðŸ§ª
from ecoscope_workflows_core.tasks.skip import any_is_empty_df
from ecoscope_workflows_core.tasks.skip import any_dependency_skipped
from ecoscope_workflows_ext_ecoscope.tasks.preprocessing import (
    relocations_to_trajectory,
)
from ecoscope_workflows_core.tasks.transformation import add_temporal_index
from ecoscope_workflows_ext_ecoscope.tasks.transformation import apply_classification
from ecoscope_workflows_core.tasks.transformation import sort_values
from ecoscope_workflows_ext_ecoscope.tasks.transformation import apply_color_map
from ecoscope_workflows_core.tasks.transformation import map_values_with_unit
from ecoscope_workflows_ext_ecoscope.tasks.results import create_polyline_layer
from ecoscope_workflows_ext_mep.tasks import create_view_state_from_gdf
from ecoscope_workflows_ext_mep.tasks import combine_map_layers
from ecoscope_workflows_ext_mep.tasks import zip_grouped_by_key
from ecoscope_workflows_ext_ecoscope.tasks.results import draw_ecomap
from ecoscope_workflows_core.tasks.io import persist_text
from ecoscope_workflows_core.tasks.results import create_map_widget_single_view
from ecoscope_workflows_core.tasks.skip import never
from ecoscope_workflows_core.tasks.results import merge_widget_views
from ecoscope_workflows_ext_ecoscope.tasks.analysis import (
    calculate_elliptical_time_density,
)

determine_season_windows = create_task_magicmock(  # ðŸ§ª
    anchor="ecoscope_workflows_ext_ecoscope.tasks.io",  # ðŸ§ª
    func_name="determine_season_windows",  # ðŸ§ª
)  # ðŸ§ª
from ecoscope_workflows_ext_mep.tasks import create_seasonal_labels
from ecoscope_workflows_ext_mep.tasks import generate_mcp_gdf
from ecoscope_workflows_ext_ecoscope.tasks.results import create_polygon_layer
from ecoscope_workflows_ext_mep.tasks import calculate_seasonal_home_range
from ecoscope_workflows_ext_ecoscope.tasks.skip import all_geometry_are_none
from ecoscope_workflows_ext_ecoscope.tasks.io import persist_df
from ecoscope_workflows_ext_mep.tasks import generate_seasonal_nsd_plot
from ecoscope_workflows_ext_mep.tasks import generate_seasonal_speed_plot
from ecoscope_workflows_ext_mep.tasks import generate_collared_seasonal_plot
from ecoscope_workflows_ext_mep.tasks import generate_seasonal_mcp_asymptote_plot
from ecoscope_workflows_ext_mep.tasks import get_subject_stats
from ecoscope_workflows_ext_mep.tasks import build_template_region_lookup
from ecoscope_workflows_ext_mep.tasks import compute_template_regions
from ecoscope_workflows_ext_mep.tasks import compute_subject_occupancy
from ecoscope_workflows_ext_custom.tasks import html_to_png
from ecoscope_workflows_core.tasks.results import gather_dashboard

from ..params import Params


def main(params: Params):
    warnings.warn("This test script should not be used in production!")  # ðŸ§ª

    params_dict = json.loads(params.model_dump_json(exclude_unset=True))

    workflow_details = (
        set_workflow_details.validate()
        .handle_errors(task_instance_id="workflow_details")
        .partial(**(params_dict.get("workflow_details") or {}))
        .call()
    )

    gee_client = (
        set_gee_connection.validate()
        .handle_errors(task_instance_id="gee_client")
        .partial(**(params_dict.get("gee_client") or {}))
        .call()
    )

    er_client = (
        set_er_connection.validate()
        .handle_errors(task_instance_id="er_client")
        .partial(**(params_dict.get("er_client") or {}))
        .call()
    )

    configure_base_maps = (
        set_base_maps.validate()
        .handle_errors(task_instance_id="configure_base_maps")
        .partial(**(params_dict.get("configure_base_maps") or {}))
        .call()
    )

    configure_grouping_strategy = (
        set_groupers.validate()
        .handle_errors(task_instance_id="configure_grouping_strategy")
        .partial(**(params_dict.get("configure_grouping_strategy") or {}))
        .call()
    )

    define_time_range = (
        set_time_range.validate()
        .handle_errors(task_instance_id="define_time_range")
        .partial(
            time_format="%d %b %Y %H:%M:%S %Z",
            **(params_dict.get("define_time_range") or {}),
        )
        .call()
    )

    create_output_dir = (
        create_directory.validate()
        .handle_errors(task_instance_id="create_output_dir")
        .partial(**(params_dict.get("create_output_dir") or {}))
        .call()
    )

    retrieve_ldx_db = (
        download_land_dx.validate()
        .handle_errors(task_instance_id="retrieve_ldx_db")
        .partial(
            path=create_output_dir,
            url="https://maraelephant.maps.arcgis.com/sharing/rest/content/items/6da0c9bdd43d4dd0ac59a4f3cd73dcab/data",
            **(params_dict.get("retrieve_ldx_db") or {}),
        )
        .call()
    )

    download_logo = (
        download_file_and_persist.validate()
        .handle_errors(task_instance_id="download_logo")
        .partial(
            output_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            overwrite_existing=False,
            unzip=True,
            **(params_dict.get("download_logo") or {}),
        )
        .call()
    )

    download_cover_page = (
        download_file_and_persist.validate()
        .handle_errors(task_instance_id="download_cover_page")
        .partial(
            url="https://www.dropbox.com/scl/fi/my6cd3fhs8wtkv34sqb0l/cover_page.pdf?rlkey=45ejxhslkmpa05nhdv6ehxnod&dl=1",
            output_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            overwrite_existing=False,
            **(params_dict.get("download_cover_page") or {}),
        )
        .call()
    )

    download_template_one = (
        download_file_and_persist.validate()
        .handle_errors(task_instance_id="download_template_one")
        .partial(
            url="https://www.dropbox.com/scl/fi/m35nrobegmfrbzeavd42g/template_1.html?rlkey=xsnkvxpsjlvd6hyozjxp4die3&st=fiobix4t&dl=0",
            output_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            overwrite_existing=False,
            **(params_dict.get("download_template_one") or {}),
        )
        .call()
    )

    download_template_two = (
        download_file_and_persist.validate()
        .handle_errors(task_instance_id="download_template_two")
        .partial(
            url="https://www.dropbox.com/scl/fi/6skjcsspv9rf5jgsyedgt/template_2.html?rlkey=lk73uq9m506ve9qsmghkk57k2&st=6lbck9k3&dl=0",
            output_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            overwrite_existing=False,
            **(params_dict.get("download_template_two") or {}),
        )
        .call()
    )

    download_template_three = (
        download_file_and_persist.validate()
        .handle_errors(task_instance_id="download_template_three")
        .partial(
            url="https://www.dropbox.com/scl/fi/44yv82b85lws8u657nuoy/template_3.html?rlkey=b0bdy09kb2oom13rvmd6r3gbc&st=550bazn1&dl=0",
            output_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            overwrite_existing=False,
            **(params_dict.get("download_template_three") or {}),
        )
        .call()
    )

    load_aoi = (
        load_landdx_aoi.validate()
        .handle_errors(task_instance_id="load_aoi")
        .partial(map_path=retrieve_ldx_db, **(params_dict.get("load_aoi") or {}))
        .call()
    )

    split_landdx_by_type = (
        split_gdf_by_column.validate()
        .handle_errors(task_instance_id="split_landdx_by_type")
        .partial(
            gdf=load_aoi,
            column="type",
            **(params_dict.get("split_landdx_by_type") or {}),
        )
        .call()
    )

    annotate_geom_types = (
        annotate_gdf_dict_with_geometry_type.validate()
        .handle_errors(task_instance_id="annotate_geom_types")
        .partial(
            gdf_dict=split_landdx_by_type,
            **(params_dict.get("annotate_geom_types") or {}),
        )
        .call()
    )

    create_styled_ldx_layers = (
        create_map_layers_from_annotated_dict.validate()
        .handle_errors(task_instance_id="create_styled_ldx_layers")
        .partial(
            annotated_dict=annotate_geom_types,
            **(params_dict.get("create_styled_ldx_layers") or {}),
        )
        .call()
    )

    subject_df = (
        get_subjects_info.validate()
        .handle_errors(task_instance_id="subject_df")
        .partial(
            client=er_client,
            include_inactive=True,
            **(params_dict.get("subject_df") or {}),
        )
        .call()
    )

    normalize_subject_info = (
        normalize_column.validate()
        .handle_errors(task_instance_id="normalize_subject_info")
        .partial(
            column="additional",
            df=subject_df,
            **(params_dict.get("normalize_subject_info") or {}),
        )
        .call()
    )

    rename_subject_cols = (
        map_columns.validate()
        .handle_errors(task_instance_id="rename_subject_cols")
        .partial(
            drop_columns=[
                "url",
                "image_url",
                "common_name",
                "content_type",
                "additional__external_id",
                "additional__tm_animal_id",
                "additional__external_name",
            ],
            retain_columns=[
                "id",
                "name",
                "hex",
                "additional__rgb",
                "additional__sex",
                "additional__Bio",
                "additional__DOB",
                "additional__notes",
                "additional__status",
                "additional__region",
                "additional__country",
                "additional__id_photo",
                "additional__distribution",
            ],
            rename_columns={
                "id": "groupby_col",
                "name": "subject_name",
                "hex": "hex_color",
                "additional__rgb": "rgb",
                "additional__Bio": "subject_bio",
                "additional__sex": "subject_sex",
                "additional__DOB": "date_of_birth",
                "additional__notes": "notes",
                "additional__status": "status",
                "additional__region": "region",
                "additional__country": "country",
                "additional__id_photo": "photo",
                "additional__distribution": "distribution",
            },
            df=normalize_subject_info,
            **(params_dict.get("rename_subject_cols") or {}),
        )
        .call()
    )

    subject_observations = (
        get_subjectgroup_observations.validate()
        .handle_errors(task_instance_id="subject_observations")
        .partial(
            client=er_client,
            time_range=define_time_range,
            raise_on_empty=False,
            include_details=False,
            include_subjectsource_details=False,
            **(params_dict.get("subject_observations") or {}),
        )
        .call()
    )

    subject_relocations = (
        process_relocations.validate()
        .handle_errors(task_instance_id="subject_relocations")
        .partial(
            observations=subject_observations,
            relocs_columns=[
                "fixtime",
                "geometry",
                "groupby_col",
                "junk_status",
                "extra__created_at",
                "extra__subject__sex",
                "extra__subject__hex",
                "extra__subject__name",
                "extra__subject__subject_subtype",
            ],
            filter_point_coords=[
                {"x": 180.0, "y": 90.0},
                {"x": 0.0, "y": 0.0},
                {"x": 1.0, "y": 1.0},
            ],
            **(params_dict.get("subject_relocations") or {}),
        )
        .call()
    )

    rename_reloc_cols = (
        map_columns.validate()
        .handle_errors(task_instance_id="rename_reloc_cols")
        .partial(
            drop_columns=[],
            retain_columns=[],
            rename_columns={
                "extra__subject__name": "subject_name",
                "extra__subject__hex": "hex_color",
                "extra__subject__sex": "subject_sex",
                "extra__subject__subject_subtype": "subject_subtype",
                "extra__created_at": "created_at",
            },
            df=subject_observations,
            **(params_dict.get("rename_reloc_cols") or {}),
        )
        .call()
    )

    compute_subject_maturity = (
        compute_maturity.validate()
        .handle_errors(task_instance_id="compute_subject_maturity")
        .partial(
            subject_df=rename_subject_cols,
            relocations_gdf=rename_reloc_cols,
            months_duration=6,
            **(params_dict.get("compute_subject_maturity") or {}),
        )
        .call()
    )

    split_subject_by_group = (
        split_groups.validate()
        .handle_errors(task_instance_id="split_subject_by_group")
        .partial(
            df=compute_subject_maturity,
            groupers=configure_grouping_strategy,
            **(params_dict.get("split_subject_by_group") or {}),
        )
        .call()
    )

    download_profile_pic = (
        download_profile_photo.validate()
        .handle_errors(task_instance_id="download_profile_pic")
        .partial(
            image_type=".png",
            column="photo",
            overwrite_existing=True,
            output_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            **(params_dict.get("download_profile_pic") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=split_subject_by_group)
    )

    download_subject_info = (
        persist_subject_info.validate()
        .handle_errors(task_instance_id="download_subject_info")
        .partial(
            maxlen=1000,
            output_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            **(params_dict.get("download_subject_info") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=split_subject_by_group)
    )

    get_events_data = (
        get_events.validate()
        .handle_errors(task_instance_id="get_events_data")
        .partial(
            client=er_client,
            time_range=define_time_range,
            include_details=True,
            raise_on_empty=True,
            event_columns=[
                "id",
                "time",
                "event_type",
                "event_category",
                "reported_by",
                "serial_number",
                "geometry",
                "event_details",
            ],
            **(params_dict.get("get_events_data") or {}),
        )
        .call()
    )

    normalize_event_details = (
        normalize_column.validate()
        .handle_errors(task_instance_id="normalize_event_details")
        .partial(
            column="event_details",
            df=get_events_data,
            **(params_dict.get("normalize_event_details") or {}),
        )
        .call()
    )

    rename_event_cols = (
        map_columns.validate()
        .handle_errors(task_instance_id="rename_event_cols")
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            drop_columns=[],
            retain_columns=[],
            rename_columns={
                "event_details__pic": "pic",
                "event_details__region": "region",
                "event_details__source": "source",
                "event_details__details": "details",
                "event_details__subject": "groupby_col",
                "event_details__source_id": "source_id",
                "event_details__subject_id": "subject_name",
                "event_details__collaring_type": "collaring_type",
                "event_details__collaring_reason": "collaring_reason",
                "event_details__collar_checked_by": "collar_checked_by",
            },
            df=normalize_event_details,
            **(params_dict.get("rename_event_cols") or {}),
        )
        .call()
    )

    split_events_by_group = (
        split_groups.validate()
        .handle_errors(task_instance_id="split_events_by_group")
        .partial(
            df=rename_event_cols,
            groupers=configure_grouping_strategy,
            **(params_dict.get("split_events_by_group") or {}),
        )
        .call()
    )

    split_relocs_by_group = (
        split_groups.validate()
        .handle_errors(task_instance_id="split_relocs_by_group")
        .partial(
            df=rename_reloc_cols,
            groupers=configure_grouping_strategy,
            **(params_dict.get("split_relocs_by_group") or {}),
        )
        .call()
    )

    convert_to_trajectories = (
        relocations_to_trajectory.validate()
        .handle_errors(task_instance_id="convert_to_trajectories")
        .partial(
            relocations=rename_reloc_cols,
            **(params_dict.get("convert_to_trajectories") or {}),
        )
        .call()
    )

    add_temporal_index_to_traj = (
        add_temporal_index.validate()
        .handle_errors(task_instance_id="add_temporal_index_to_traj")
        .partial(
            df=convert_to_trajectories,
            time_col="segment_start",
            groupers=configure_grouping_strategy,
            cast_to_datetime=True,
            format="mixed",
            **(params_dict.get("add_temporal_index_to_traj") or {}),
        )
        .call()
    )

    classify_trajectory_speed_bins = (
        apply_classification.validate()
        .handle_errors(task_instance_id="classify_trajectory_speed_bins")
        .partial(
            df=add_temporal_index_to_traj,
            input_column_name="speed_kmhr",
            output_column_name="speed_bins",
            classification_options={"scheme": "equal_interval", "k": 6},
            label_options={"label_ranges": False, "label_decimals": 1},
            **(params_dict.get("classify_trajectory_speed_bins") or {}),
        )
        .call()
    )

    rename_trajectory_cols = (
        map_columns.validate()
        .handle_errors(task_instance_id="rename_trajectory_cols")
        .partial(
            drop_columns=[],
            retain_columns=[],
            rename_columns={
                "extra__subject_name": "subject_name",
                "extra__hex_color": "hex_color",
                "extra__subject_sex": "subject_sex",
                "extra__subject_subtype": "subject_subtype",
            },
            df=classify_trajectory_speed_bins,
            **(params_dict.get("rename_trajectory_cols") or {}),
        )
        .call()
    )

    split_trajs_by_group = (
        split_groups.validate()
        .handle_errors(task_instance_id="split_trajs_by_group")
        .partial(
            df=rename_trajectory_cols,
            groupers=configure_grouping_strategy,
            **(params_dict.get("split_trajs_by_group") or {}),
        )
        .call()
    )

    sort_trajs_by_speed = (
        sort_values.validate()
        .handle_errors(task_instance_id="sort_trajs_by_speed")
        .partial(
            column_name="speed_bins",
            na_position="last",
            **(params_dict.get("sort_trajs_by_speed") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=split_trajs_by_group)
    )

    apply_speed_colormap = (
        apply_color_map.validate()
        .handle_errors(task_instance_id="apply_speed_colormap")
        .partial(
            input_column_name="speed_bins",
            output_column_name="speed_bins_colormap",
            colormap=["#1a9850", "#91cf60", "#d9ef8b", "#fee08b", "#fc8d59", "#d73027"],
            **(params_dict.get("apply_speed_colormap") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=sort_trajs_by_speed)
    )

    format_speed_bin_labels = (
        map_values_with_unit.validate()
        .handle_errors(task_instance_id="format_speed_bin_labels")
        .partial(
            input_column_name="speed_bins",
            output_column_name="speed_bins_formatted",
            original_unit="km/h",
            new_unit="km/h",
            decimal_places=1,
            **(params_dict.get("format_speed_bin_labels") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=apply_speed_colormap)
    )

    format_speed_values = (
        map_values_with_unit.validate()
        .handle_errors(task_instance_id="format_speed_values")
        .partial(
            input_column_name="speed_kmhr",
            output_column_name="speed_kmhr",
            original_unit="km/h",
            new_unit="km/h",
            decimal_places=1,
            **(params_dict.get("format_speed_values") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=format_speed_bin_labels)
    )

    generate_speedmap_layers = (
        create_polyline_layer.validate()
        .handle_errors(task_instance_id="generate_speedmap_layers")
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            layer_style={"color_column": "speed_bins_colormap"},
            legend={
                "label_column": "speed_bins_formatted",
                "color_column": "speed_bins_colormap",
            },
            tooltip_columns=[
                "subject_name",
                "subject_sex",
                "subject_subtype",
                "speed_kmhr",
                "speed_bins",
                "dist_meters",
            ],
            **(params_dict.get("generate_speedmap_layers") or {}),
        )
        .mapvalues(argnames=["geodataframe"], argvalues=format_speed_values)
    )

    zoom_traj_view = (
        create_view_state_from_gdf.validate()
        .handle_errors(task_instance_id="zoom_traj_view")
        .partial(pitch=0, bearing=0, **(params_dict.get("zoom_traj_view") or {}))
        .mapvalues(argnames=["gdf"], argvalues=format_speed_values)
    )

    combine_landdx_speed_layers = (
        combine_map_layers.validate()
        .handle_errors(task_instance_id="combine_landdx_speed_layers")
        .partial(
            static_layers=create_styled_ldx_layers,
            **(params_dict.get("combine_landdx_speed_layers") or {}),
        )
        .mapvalues(argnames=["grouped_layers"], argvalues=generate_speedmap_layers)
    )

    speedvalues_view_zip = (
        zip_grouped_by_key.validate()
        .handle_errors(task_instance_id="speedvalues_view_zip")
        .partial(
            left=combine_landdx_speed_layers,
            right=zoom_traj_view,
            **(params_dict.get("speedvalues_view_zip") or {}),
        )
        .call()
    )

    draw_speedmap = (
        draw_ecomap.validate()
        .handle_errors(task_instance_id="draw_speedmap")
        .partial(
            tile_layers=configure_base_maps,
            north_arrow_style={"placement": "top-left"},
            legend_style={"placement": "bottom-right", "title": "Speed Values(Km/h)"},
            static=False,
            title=None,
            max_zoom=20,
            **(params_dict.get("draw_speedmap") or {}),
        )
        .mapvalues(
            argnames=["geo_layers", "view_state"], argvalues=speedvalues_view_zip
        )
    )

    persist_speed_ecomap_urls = (
        persist_text.validate()
        .handle_errors(task_instance_id="persist_speed_ecomap_urls")
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            **(params_dict.get("persist_speed_ecomap_urls") or {}),
        )
        .mapvalues(argnames=["text"], argvalues=draw_speedmap)
    )

    create_speed_ecomap_widgets = (
        create_map_widget_single_view.validate()
        .handle_errors(task_instance_id="create_speed_ecomap_widgets")
        .skipif(
            conditions=[
                never,
            ],
            unpack_depth=1,
        )
        .partial(
            title="Speedmap", **(params_dict.get("create_speed_ecomap_widgets") or {})
        )
        .map(argnames=["view", "data"], argvalues=persist_speed_ecomap_urls)
    )

    merge_speed_ecomap_widgets = (
        merge_widget_views.validate()
        .handle_errors(task_instance_id="merge_speed_ecomap_widgets")
        .partial(
            widgets=create_speed_ecomap_widgets,
            **(params_dict.get("merge_speed_ecomap_widgets") or {}),
        )
        .call()
    )

    generate_etd = (
        calculate_elliptical_time_density.validate()
        .handle_errors(task_instance_id="generate_etd")
        .partial(
            crs="ESRI:53042",
            percentiles=[50.0, 60.0, 70.0, 80.0, 90.0, 95.0, 99.9],
            nodata_value="nan",
            band_count=1,
            **(params_dict.get("generate_etd") or {}),
        )
        .mapvalues(argnames=["trajectory_gdf"], argvalues=split_trajs_by_group)
    )

    determine_seasonal_windows = (
        determine_season_windows.validate()
        .handle_errors(task_instance_id="determine_seasonal_windows")
        .partial(
            client=gee_client,
            time_range=define_time_range,
            **(params_dict.get("determine_seasonal_windows") or {}),
        )
        .mapvalues(argnames=["roi"], argvalues=generate_etd)
    )

    zip_etd_and_grouped_trajs = (
        zip_grouped_by_key.validate()
        .handle_errors(task_instance_id="zip_etd_and_grouped_trajs")
        .partial(
            left=determine_seasonal_windows,
            right=split_trajs_by_group,
            **(params_dict.get("zip_etd_and_grouped_trajs") or {}),
        )
        .call()
    )

    add_season_labels = (
        create_seasonal_labels.validate()
        .handle_errors(task_instance_id="add_season_labels")
        .partial(**(params_dict.get("add_season_labels") or {}))
        .mapvalues(
            argnames=["total_percentiles", "traj"], argvalues=zip_etd_and_grouped_trajs
        )
    )

    calculate_mcp = (
        generate_mcp_gdf.validate()
        .handle_errors(task_instance_id="calculate_mcp")
        .partial(planar_crs="ESRI:53042", **(params_dict.get("calculate_mcp") or {}))
        .mapvalues(argnames=["gdf"], argvalues=split_trajs_by_group)
    )

    apply_etd_percentile_colormap = (
        apply_color_map.validate()
        .handle_errors(task_instance_id="apply_etd_percentile_colormap")
        .partial(
            input_column_name="percentile",
            colormap="RdYlGn",
            output_column_name="percentile_colormap",
            **(params_dict.get("apply_etd_percentile_colormap") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=generate_etd)
    )

    generate_etd_ecomap_layers = (
        create_polygon_layer.validate()
        .handle_errors(task_instance_id="generate_etd_ecomap_layers")
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            layer_style={"fill_color_column": "percentile_colormap", "opacity": 0.55},
            legend={
                "label_column": "percentile",
                "color_column": "percentile_colormap",
            },
            tooltip_columns=["percentile"],
            **(params_dict.get("generate_etd_ecomap_layers") or {}),
        )
        .mapvalues(argnames=["geodataframe"], argvalues=apply_etd_percentile_colormap)
    )

    zoom_hr_view = (
        create_view_state_from_gdf.validate()
        .handle_errors(task_instance_id="zoom_hr_view")
        .partial(pitch=0, bearing=0, **(params_dict.get("zoom_hr_view") or {}))
        .mapvalues(argnames=["gdf"], argvalues=apply_etd_percentile_colormap)
    )

    combine_landdx_hr_ecomap_layers = (
        combine_map_layers.validate()
        .handle_errors(task_instance_id="combine_landdx_hr_ecomap_layers")
        .partial(
            static_layers=create_styled_ldx_layers,
            **(params_dict.get("combine_landdx_hr_ecomap_layers") or {}),
        )
        .mapvalues(argnames=["grouped_layers"], argvalues=generate_etd_ecomap_layers)
    )

    hr_view_zip = (
        zip_grouped_by_key.validate()
        .handle_errors(task_instance_id="hr_view_zip")
        .partial(
            left=combine_landdx_hr_ecomap_layers,
            right=zoom_hr_view,
            **(params_dict.get("hr_view_zip") or {}),
        )
        .call()
    )

    draw_hr_ecomaps = (
        draw_ecomap.validate()
        .handle_errors(task_instance_id="draw_hr_ecomaps")
        .partial(
            tile_layers=configure_base_maps,
            north_arrow_style={"placement": "top-left"},
            legend_style={"placement": "bottom-right", "title": "Home Range Metrics"},
            static=False,
            title=None,
            max_zoom=20,
            **(params_dict.get("draw_hr_ecomaps") or {}),
        )
        .mapvalues(argnames=["geo_layers", "view_state"], argvalues=hr_view_zip)
    )

    persist_hr_ecomap_urls = (
        persist_text.validate()
        .handle_errors(task_instance_id="persist_hr_ecomap_urls")
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            **(params_dict.get("persist_hr_ecomap_urls") or {}),
        )
        .mapvalues(argnames=["text"], argvalues=draw_hr_ecomaps)
    )

    create_hr_ecomap_widgets = (
        create_map_widget_single_view.validate()
        .handle_errors(task_instance_id="create_hr_ecomap_widgets")
        .skipif(
            conditions=[
                never,
            ],
            unpack_depth=1,
        )
        .partial(
            title="Home Range Ecomap",
            **(params_dict.get("create_hr_ecomap_widgets") or {}),
        )
        .map(argnames=["view", "data"], argvalues=persist_hr_ecomap_urls)
    )

    merge_hr_ecomap_widgets = (
        merge_widget_views.validate()
        .handle_errors(task_instance_id="merge_hr_ecomap_widgets")
        .partial(
            widgets=create_hr_ecomap_widgets,
            **(params_dict.get("merge_hr_ecomap_widgets") or {}),
        )
        .call()
    )

    seasonal_home_range = (
        calculate_seasonal_home_range.validate()
        .handle_errors(task_instance_id="seasonal_home_range")
        .skipif(
            conditions=[
                any_is_empty_df,
                all_geometry_are_none,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            groupby_cols=["subject_name", "season"],
            **(params_dict.get("seasonal_home_range") or {}),
        )
        .mapvalues(argnames=["gdf"], argvalues=add_season_labels)
    )

    season_colormap = (
        apply_color_map.validate()
        .handle_errors(task_instance_id="season_colormap")
        .partial(
            input_column_name="season",
            output_column_name="season_colormap",
            colormap=["#f57c00", "#4cf3f7"],
            **(params_dict.get("season_colormap") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=seasonal_home_range)
    )

    season_etd_map_layer = (
        create_polygon_layer.validate()
        .handle_errors(task_instance_id="season_etd_map_layer")
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            layer_style={"fill_color_column": "season_colormap", "opacity": 0.65},
            legend={"label_column": "season", "color_column": "season_colormap"},
            tooltip_columns=["percentile"],
            **(params_dict.get("season_etd_map_layer") or {}),
        )
        .mapvalues(argnames=["geodataframe"], argvalues=season_colormap)
    )

    generate_mcp_layers = (
        create_polygon_layer.validate()
        .handle_errors(task_instance_id="generate_mcp_layers")
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            layer_style={
                "get_fill_color": "#FFFFFF00",
                "get_line_color": "#dc143c",
                "opacity": 0.55,
                "stroked": True,
            },
            legend={"labels": ["mcp"], "colors": ["#dc143c"]},
            tooltip_columns=["area_km2"],
            **(params_dict.get("generate_mcp_layers") or {}),
        )
        .mapvalues(argnames=["geodataframe"], argvalues=calculate_mcp)
    )

    zip_season_mcp_hr = (
        zip_grouped_by_key.validate()
        .handle_errors(task_instance_id="zip_season_mcp_hr")
        .partial(
            left=generate_mcp_layers,
            right=season_etd_map_layer,
            **(params_dict.get("zip_season_mcp_hr") or {}),
        )
        .call()
    )

    zoom_season_view = (
        create_view_state_from_gdf.validate()
        .handle_errors(task_instance_id="zoom_season_view")
        .partial(pitch=0, bearing=0, **(params_dict.get("zoom_season_view") or {}))
        .mapvalues(argnames=["gdf"], argvalues=season_colormap)
    )

    comb_season_map_layers = (
        combine_map_layers.validate()
        .handle_errors(task_instance_id="comb_season_map_layers")
        .partial(
            static_layers=create_styled_ldx_layers,
            **(params_dict.get("comb_season_map_layers") or {}),
        )
        .mapvalues(argnames=["grouped_layers"], argvalues=zip_season_mcp_hr)
    )

    seasons_view_zip = (
        zip_grouped_by_key.validate()
        .handle_errors(task_instance_id="seasons_view_zip")
        .partial(
            left=comb_season_map_layers,
            right=zoom_season_view,
            **(params_dict.get("seasons_view_zip") or {}),
        )
        .call()
    )

    seasonal_ecomap = (
        draw_ecomap.validate()
        .handle_errors(task_instance_id="seasonal_ecomap")
        .partial(
            tile_layers=configure_base_maps,
            north_arrow_style={"placement": "top-left"},
            legend_style={"placement": "bottom-right", "title": "Seasons"},
            static=False,
            title=None,
            max_zoom=20,
            **(params_dict.get("seasonal_ecomap") or {}),
        )
        .mapvalues(argnames=["geo_layers", "view_state"], argvalues=seasons_view_zip)
    )

    season_etd_ecomap_html_url = (
        persist_text.validate()
        .handle_errors(task_instance_id="season_etd_ecomap_html_url")
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            **(params_dict.get("season_etd_ecomap_html_url") or {}),
        )
        .mapvalues(argnames=["text"], argvalues=seasonal_ecomap)
    )

    season_etd_widgets_single_view = (
        create_map_widget_single_view.validate()
        .handle_errors(task_instance_id="season_etd_widgets_single_view")
        .skipif(
            conditions=[
                never,
            ],
            unpack_depth=1,
        )
        .partial(
            title="Subject Group Home Range Map (Seasons)",
            **(params_dict.get("season_etd_widgets_single_view") or {}),
        )
        .map(argnames=["view", "data"], argvalues=season_etd_ecomap_html_url)
    )

    season_grouped_map_widget = (
        merge_widget_views.validate()
        .handle_errors(task_instance_id="season_grouped_map_widget")
        .partial(
            widgets=season_etd_widgets_single_view,
            **(params_dict.get("season_grouped_map_widget") or {}),
        )
        .call()
    )

    persist_subject_season_wins = (
        persist_df.validate()
        .handle_errors(task_instance_id="persist_subject_season_wins")
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            **(params_dict.get("persist_subject_season_wins") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=determine_seasonal_windows)
    )

    zip_nsd_values = (
        zip_grouped_by_key.validate()
        .handle_errors(task_instance_id="zip_nsd_values")
        .partial(
            left=split_relocs_by_group,
            right=persist_subject_season_wins,
            **(params_dict.get("zip_nsd_values") or {}),
        )
        .call()
    )

    generate_nsd_plot = (
        generate_seasonal_nsd_plot.validate()
        .handle_errors(task_instance_id="generate_nsd_plot")
        .partial(**(params_dict.get("generate_nsd_plot") or {}))
        .mapvalues(argnames=["gdf", "seasons_df"], argvalues=zip_nsd_values)
    )

    persist_nsd_html_urls = (
        persist_text.validate()
        .handle_errors(task_instance_id="persist_nsd_html_urls")
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            **(params_dict.get("persist_nsd_html_urls") or {}),
        )
        .mapvalues(argnames=["text"], argvalues=generate_nsd_plot)
    )

    nsd_plot_widgets_single_view = (
        create_map_widget_single_view.validate()
        .handle_errors(task_instance_id="nsd_plot_widgets_single_view")
        .skipif(
            conditions=[
                never,
            ],
            unpack_depth=1,
        )
        .partial(
            title="Net Square Displacement (NSD)",
            **(params_dict.get("nsd_plot_widgets_single_view") or {}),
        )
        .map(argnames=["view", "data"], argvalues=persist_nsd_html_urls)
    )

    grouped_nsd_plot_widget = (
        merge_widget_views.validate()
        .handle_errors(task_instance_id="grouped_nsd_plot_widget")
        .partial(
            widgets=nsd_plot_widgets_single_view,
            **(params_dict.get("grouped_nsd_plot_widget") or {}),
        )
        .call()
    )

    zip_speed_values = (
        zip_grouped_by_key.validate()
        .handle_errors(task_instance_id="zip_speed_values")
        .partial(
            left=split_relocs_by_group,
            right=persist_subject_season_wins,
            **(params_dict.get("zip_speed_values") or {}),
        )
        .call()
    )

    generate_speed_plot = (
        generate_seasonal_speed_plot.validate()
        .handle_errors(task_instance_id="generate_speed_plot")
        .partial(**(params_dict.get("generate_speed_plot") or {}))
        .mapvalues(argnames=["gdf", "seasons_df"], argvalues=zip_speed_values)
    )

    persist_speed_html_urls = (
        persist_text.validate()
        .handle_errors(task_instance_id="persist_speed_html_urls")
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            **(params_dict.get("persist_speed_html_urls") or {}),
        )
        .mapvalues(argnames=["text"], argvalues=generate_speed_plot)
    )

    speed_plot_widgets_single_view = (
        create_map_widget_single_view.validate()
        .handle_errors(task_instance_id="speed_plot_widgets_single_view")
        .skipif(
            conditions=[
                never,
            ],
            unpack_depth=1,
        )
        .partial(
            title="Speed", **(params_dict.get("speed_plot_widgets_single_view") or {})
        )
        .map(argnames=["view", "data"], argvalues=persist_speed_html_urls)
    )

    grouped_speed_plot_widget = (
        merge_widget_views.validate()
        .handle_errors(task_instance_id="grouped_speed_plot_widget")
        .partial(
            widgets=speed_plot_widgets_single_view,
            **(params_dict.get("grouped_speed_plot_widget") or {}),
        )
        .call()
    )

    zip_collared_subject_values = (
        zip_grouped_by_key.validate()
        .handle_errors(task_instance_id="zip_collared_subject_values")
        .partial(
            left=split_relocs_by_group,
            right=persist_subject_season_wins,
            **(params_dict.get("zip_collared_subject_values") or {}),
        )
        .call()
    )

    generate_colared_subject_plot = (
        generate_collared_seasonal_plot.validate()
        .handle_errors(task_instance_id="generate_colared_subject_plot")
        .partial(
            events_gdf=rename_event_cols,
            filter_col="subject_name",
            **(params_dict.get("generate_colared_subject_plot") or {}),
        )
        .mapvalues(
            argnames=["relocations_gdf", "seasons_df"],
            argvalues=zip_collared_subject_values,
        )
    )

    persist_collared_subject_plots = (
        persist_text.validate()
        .handle_errors(task_instance_id="persist_collared_subject_plots")
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            **(params_dict.get("persist_collared_subject_plots") or {}),
        )
        .mapvalues(argnames=["text"], argvalues=generate_colared_subject_plot)
    )

    collared_widget_view = (
        create_map_widget_single_view.validate()
        .handle_errors(task_instance_id="collared_widget_view")
        .skipif(
            conditions=[
                never,
            ],
            unpack_depth=1,
        )
        .partial(
            title="Collared Subject Plot",
            **(params_dict.get("collared_widget_view") or {}),
        )
        .map(argnames=["view", "data"], argvalues=persist_collared_subject_plots)
    )

    grouped_collared_widget = (
        merge_widget_views.validate()
        .handle_errors(task_instance_id="grouped_collared_widget")
        .partial(
            widgets=collared_widget_view,
            **(params_dict.get("grouped_collared_widget") or {}),
        )
        .call()
    )

    zip_mcp_asymp_values = (
        zip_grouped_by_key.validate()
        .handle_errors(task_instance_id="zip_mcp_asymp_values")
        .partial(
            left=split_relocs_by_group,
            right=persist_subject_season_wins,
            **(params_dict.get("zip_mcp_asymp_values") or {}),
        )
        .call()
    )

    generate_mcp_asymp_plot = (
        generate_seasonal_mcp_asymptote_plot.validate()
        .handle_errors(task_instance_id="generate_mcp_asymp_plot")
        .partial(**(params_dict.get("generate_mcp_asymp_plot") or {}))
        .mapvalues(argnames=["gdf", "seasons_df"], argvalues=zip_mcp_asymp_values)
    )

    persist_mcp_html_urls = (
        persist_text.validate()
        .handle_errors(task_instance_id="persist_mcp_html_urls")
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            **(params_dict.get("persist_mcp_html_urls") or {}),
        )
        .mapvalues(argnames=["text"], argvalues=generate_mcp_asymp_plot)
    )

    mcp_plot_widgets_single_view = (
        create_map_widget_single_view.validate()
        .handle_errors(task_instance_id="mcp_plot_widgets_single_view")
        .skipif(
            conditions=[
                never,
            ],
            unpack_depth=1,
        )
        .partial(
            title="MCP Asymptote",
            **(params_dict.get("mcp_plot_widgets_single_view") or {}),
        )
        .map(argnames=["view", "data"], argvalues=persist_mcp_html_urls)
    )

    grouped_mcp_plot_widget = (
        merge_widget_views.validate()
        .handle_errors(task_instance_id="grouped_mcp_plot_widget")
        .partial(
            widgets=mcp_plot_widgets_single_view,
            **(params_dict.get("grouped_mcp_plot_widget") or {}),
        )
        .call()
    )

    zip_traj_etd_gdf = (
        zip_grouped_by_key.validate()
        .handle_errors(task_instance_id="zip_traj_etd_gdf")
        .partial(
            left=generate_etd,
            right=split_trajs_by_group,
            **(params_dict.get("zip_traj_etd_gdf") or {}),
        )
        .call()
    )

    generate_subject_stats = (
        get_subject_stats.validate()
        .handle_errors(task_instance_id="generate_subject_stats")
        .partial(
            output_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            subject_df=compute_subject_maturity,
            groupby_col="subject_name",
            **(params_dict.get("generate_subject_stats") or {}),
        )
        .mapvalues(argnames=["etd_df", "traj_gdf"], argvalues=zip_traj_etd_gdf)
    )

    load_unfiltered_ldx = (
        load_landdx_aoi.validate()
        .handle_errors(task_instance_id="load_unfiltered_ldx")
        .partial(
            map_path=retrieve_ldx_db, **(params_dict.get("load_unfiltered_ldx") or {})
        )
        .call()
    )

    zip_etd_subjects = (
        zip_grouped_by_key.validate()
        .handle_errors(task_instance_id="zip_etd_subjects")
        .partial(
            left=generate_etd,
            right=split_subject_by_group,
            **(params_dict.get("zip_etd_subjects") or {}),
        )
        .call()
    )

    build_region_lookup = (
        build_template_region_lookup.validate()
        .handle_errors(task_instance_id="build_region_lookup")
        .partial(
            gdf=load_unfiltered_ldx, **(params_dict.get("build_region_lookup") or {})
        )
        .call()
    )

    comp_template_regions = (
        compute_template_regions.validate()
        .handle_errors(task_instance_id="comp_template_regions")
        .partial(
            geodataframe=load_unfiltered_ldx,
            template_lookup=build_region_lookup,
            crs="ESRI:53042",
            **(params_dict.get("comp_template_regions") or {}),
        )
        .call()
    )

    process_subject_occupancy = (
        compute_subject_occupancy.validate()
        .handle_errors(task_instance_id="process_subject_occupancy")
        .partial(
            crs="ESRI:53042",
            regions_gdf=comp_template_regions,
            output_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            **(params_dict.get("process_subject_occupancy") or {}),
        )
        .mapvalues(argnames=["etd_gdf", "subjects_df"], argvalues=zip_etd_subjects)
    )

    convt_range_html_png = (
        html_to_png.validate()
        .handle_errors(task_instance_id="convt_range_html_png")
        .partial(
            output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            config={"wait_for_timeout": 100, "width": 765, "height": 525},
            **(params_dict.get("convt_range_html_png") or {}),
        )
        .mapvalues(argnames=["html_path"], argvalues=persist_hr_ecomap_urls)
    )

    convt_speedmap_html_png = (
        html_to_png.validate()
        .handle_errors(task_instance_id="convt_speedmap_html_png")
        .partial(
            output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            config={"wait_for_timeout": 100, "width": 765, "height": 525},
            **(params_dict.get("convt_speedmap_html_png") or {}),
        )
        .mapvalues(argnames=["html_path"], argvalues=persist_speed_ecomap_urls)
    )

    convt_seasons_html_png = (
        html_to_png.validate()
        .handle_errors(task_instance_id="convt_seasons_html_png")
        .partial(
            output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            config={"wait_for_timeout": 100, "width": 602, "height": 855},
            **(params_dict.get("convt_seasons_html_png") or {}),
        )
        .mapvalues(argnames=["html_path"], argvalues=season_etd_ecomap_html_url)
    )

    convt_nsdp_html_png = (
        html_to_png.validate()
        .handle_errors(task_instance_id="convt_nsdp_html_png")
        .partial(
            output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            config={"wait_for_timeout": 100, "width": 2238, "height": 450},
            **(params_dict.get("convt_nsdp_html_png") or {}),
        )
        .mapvalues(argnames=["html_path"], argvalues=persist_nsd_html_urls)
    )

    convt_mcp_html_png = (
        html_to_png.validate()
        .handle_errors(task_instance_id="convt_mcp_html_png")
        .partial(
            output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            config={"wait_for_timeout": 10, "width": 2238, "height": 450},
            **(params_dict.get("convt_mcp_html_png") or {}),
        )
        .mapvalues(argnames=["html_path"], argvalues=persist_mcp_html_urls)
    )

    convt_speedp_html_png = (
        html_to_png.validate()
        .handle_errors(task_instance_id="convt_speedp_html_png")
        .partial(
            output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            config={"wait_for_timeout": 10, "width": 2238, "height": 450},
            **(params_dict.get("convt_speedp_html_png") or {}),
        )
        .mapvalues(argnames=["html_path"], argvalues=persist_speed_html_urls)
    )

    convt_colev_html_png = (
        html_to_png.validate()
        .handle_errors(task_instance_id="convt_colev_html_png")
        .partial(
            output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            config={"wait_for_timeout": 10, "width": 2238, "height": 450},
            **(params_dict.get("convt_colev_html_png") or {}),
        )
        .mapvalues(argnames=["html_path"], argvalues=persist_collared_subject_plots)
    )

    collared_report_template = (
        gather_dashboard.validate()
        .handle_errors(task_instance_id="collared_report_template")
        .partial(
            details=workflow_details,
            widgets=[
                merge_speed_ecomap_widgets,
                merge_hr_ecomap_widgets,
                season_grouped_map_widget,
                grouped_nsd_plot_widget,
                grouped_speed_plot_widget,
                grouped_collared_widget,
                grouped_mcp_plot_widget,
            ],
            time_range=define_time_range,
            groupers=configure_grouping_strategy,
            **(params_dict.get("collared_report_template") or {}),
        )
        .call()
    )

    return collared_report_template
