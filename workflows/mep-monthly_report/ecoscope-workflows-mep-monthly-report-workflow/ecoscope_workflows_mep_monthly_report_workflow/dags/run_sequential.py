# AUTOGENERATED BY ECOSCOPE-WORKFLOWS; see fingerprint in README.md for details
import json
import os

from ecoscope_workflows_core.tasks.config import set_string_var as set_string_var
from ecoscope_workflows_core.tasks.config import (
    set_workflow_details as set_workflow_details,
)
from ecoscope_workflows_core.tasks.filter import set_time_range as set_time_range
from ecoscope_workflows_core.tasks.groupby import set_groupers as set_groupers
from ecoscope_workflows_core.tasks.io import persist_text as persist_text
from ecoscope_workflows_core.tasks.io import set_er_connection as set_er_connection
from ecoscope_workflows_core.tasks.io import set_gee_connection as set_gee_connection
from ecoscope_workflows_core.tasks.results import gather_dashboard as gather_dashboard
from ecoscope_workflows_core.tasks.skip import (
    any_dependency_skipped as any_dependency_skipped,
)
from ecoscope_workflows_core.tasks.skip import any_is_empty_df as any_is_empty_df
from ecoscope_workflows_core.tasks.transformation import (
    add_temporal_index as add_temporal_index,
)
from ecoscope_workflows_core.tasks.transformation import sort_values as sort_values
from ecoscope_workflows_ext_custom.tasks.io import html_to_png as html_to_png
from ecoscope_workflows_ext_custom.tasks.io import load_df as load_df
from ecoscope_workflows_ext_custom.tasks.results import (
    create_path_layer as create_path_layer,
)
from ecoscope_workflows_ext_custom.tasks.results import (
    create_scatterplot_layer as create_scatterplot_layer,
)
from ecoscope_workflows_ext_custom.tasks.results import draw_map as draw_map
from ecoscope_workflows_ext_custom.tasks.results import (
    set_base_maps_pydeck as set_base_maps_pydeck,
)
from ecoscope_workflows_ext_custom.tasks.transformation import (
    drop_null_geometry as drop_null_geometry_1,
)
from ecoscope_workflows_ext_ecoscope.tasks.io import get_events as get_events
from ecoscope_workflows_ext_ecoscope.tasks.io import (
    get_patrol_observations as get_patrol_observations,
)
from ecoscope_workflows_ext_ecoscope.tasks.io import (
    get_subjectgroup_observations as get_subjectgroup_observations,
)
from ecoscope_workflows_ext_ecoscope.tasks.io import persist_df as persist_df
from ecoscope_workflows_ext_ecoscope.tasks.preprocessing import (
    process_relocations as process_relocations,
)
from ecoscope_workflows_ext_ecoscope.tasks.preprocessing import (
    relocations_to_trajectory as relocations_to_trajectory,
)
from ecoscope_workflows_ext_ecoscope.tasks.transformation import (
    apply_classification as apply_classification,
)
from ecoscope_workflows_ext_ecoscope.tasks.transformation import (
    apply_color_map as apply_color_map,
)
from ecoscope_workflows_ext_mep.tasks import compile_sitrep as compile_sitrep
from ecoscope_workflows_ext_mep.tasks import (
    create__mep_context_page as create__mep_context_page,
)
from ecoscope_workflows_ext_mep.tasks import (
    create_mep_monthly_context as create_mep_monthly_context,
)
from ecoscope_workflows_ext_mep.tasks import (
    create_monthly_ctx_cover as create_monthly_ctx_cover,
)
from ecoscope_workflows_ext_mep.tasks import get_previous_period as get_previous_period
from ecoscope_workflows_ext_mep.tasks import (
    get_sitrep_event_config as get_sitrep_event_config,
)
from ecoscope_workflows_ext_mep.tasks import (
    process_aoi_ndvi_charts as process_aoi_ndvi_charts,
)
from ecoscope_workflows_ext_mep.tasks import (
    process_collar_voltage_charts as process_collar_voltage_charts,
)
from ecoscope_workflows_ext_mnc.tasks import (
    exclude_geom_outliers as exclude_geom_outliers,
)
from ecoscope_workflows_ext_ste.tasks import (
    fetch_and_persist_file as fetch_and_persist_file,
)
from ecoscope_workflows_ext_ste.tasks import filter_df_cols as filter_df_cols
from ecoscope_workflows_ext_ste.tasks import merge_mapbook_files as merge_mapbook_files
from ecoscope_workflows_ext_ste.tasks import transform_gdf_crs as transform_gdf_crs
from ecoscope_workflows_ext_ste.tasks import view_state_deck_gdf as view_state_deck_gdf

from ..params import Params


def main(params: Params):
    params_dict = json.loads(params.model_dump_json(exclude_unset=True))

    workflow_details = (
        set_workflow_details.validate()
        .set_task_instance_id("workflow_details")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(**(params_dict.get("workflow_details") or {}))
        .call()
    )

    time_range = (
        set_time_range.validate()
        .set_task_instance_id("time_range")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(**(params_dict.get("time_range") or {}))
        .call()
    )

    groupers = (
        set_groupers.validate()
        .set_task_instance_id("groupers")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(groupers=[], **(params_dict.get("groupers") or {}))
        .call()
    )

    configure_base_maps = (
        set_base_maps_pydeck.validate()
        .set_task_instance_id("configure_base_maps")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(**(params_dict.get("configure_base_maps") or {}))
        .call()
    )

    er_client_name = (
        set_er_connection.validate()
        .set_task_instance_id("er_client_name")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(**(params_dict.get("er_client_name") or {}))
        .call()
    )

    gee_project_name = (
        set_gee_connection.validate()
        .set_task_instance_id("gee_project_name")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(**(params_dict.get("gee_project_name") or {}))
        .call()
    )

    get_events_data = (
        get_events.validate()
        .set_task_instance_id("get_events_data")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            client=er_client_name,
            time_range=time_range,
            event_columns=[
                "id",
                "time",
                "event_type",
                "event_category",
                "reported_by",
                "serial_number",
                "geometry",
                "created_at",
                "event_details",
            ],
            event_types=["mep_elephant_sighting"],
            raise_on_empty=True,
            include_details=True,
            include_updates=False,
            include_related_events=False,
            include_null_geometry=False,
            include_display_values=False,
            **(params_dict.get("get_events_data") or {}),
        )
        .call()
    )

    exclude_mep_outliers = (
        exclude_geom_outliers.validate()
        .set_task_instance_id("exclude_mep_outliers")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=get_events_data,
            z_threshold=3,
            **(params_dict.get("exclude_mep_outliers") or {}),
        )
        .call()
    )

    remove_mep_invalid_geoms = (
        drop_null_geometry_1.validate()
        .set_task_instance_id("remove_mep_invalid_geoms")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            gdf=exclude_mep_outliers,
            geometry_column="geometry",
            **(params_dict.get("remove_mep_invalid_geoms") or {}),
        )
        .call()
    )

    generate_mb_layers = (
        create_scatterplot_layer.validate()
        .set_task_instance_id("generate_mb_layers")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            layer_style={
                "get_fill_color": [85, 107, 47],
                "get_line_color": [0, 0, 0, 200],
                "get_line_width": 0.55,
                "get_radius": 3.55,
                "opacity": 0.75,
                "stroked": True,
            },
            legend={
                "title": "Legend",
                "values": [{"label": "Elephant sightings", "color": "#556b2f"}],
            },
            geodataframe=remove_mep_invalid_geoms,
            **(params_dict.get("generate_mb_layers") or {}),
        )
        .call()
    )

    sighting_zoom_value = (
        view_state_deck_gdf.validate()
        .set_task_instance_id("sighting_zoom_value")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            pitch=0,
            bearing=0,
            gdf=remove_mep_invalid_geoms,
            **(params_dict.get("sighting_zoom_value") or {}),
        )
        .call()
    )

    draw_sightings_map = (
        draw_map.validate()
        .set_task_instance_id("draw_sightings_map")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            tile_layers=configure_base_maps,
            static=False,
            title=None,
            max_zoom=10,
            legend_style={"placement": "bottom-right"},
            geo_layers=generate_mb_layers,
            view_state=sighting_zoom_value,
            **(params_dict.get("draw_sightings_map") or {}),
        )
        .call()
    )

    persist_sightings_urls = (
        persist_text.validate()
        .set_task_instance_id("persist_sightings_urls")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            text=draw_sightings_map,
            filename="elephant_sightings_map.html",
            **(params_dict.get("persist_sightings_urls") or {}),
        )
        .call()
    )

    subject_group_var = (
        set_string_var.validate()
        .set_task_instance_id("subject_group_var")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(**(params_dict.get("subject_group_var") or {}))
        .call()
    )

    subject_observations = (
        get_subjectgroup_observations.validate()
        .set_task_instance_id("subject_observations")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            filter="clean",
            client=er_client_name,
            time_range=time_range,
            subject_group_name=subject_group_var,
            raise_on_empty=False,
            include_details=True,
            include_subjectsource_details=True,
            **(params_dict.get("subject_observations") or {}),
        )
        .call()
    )

    subject_reloc = (
        process_relocations.validate()
        .set_task_instance_id("subject_reloc")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            observations=subject_observations,
            relocs_columns=[
                "groupby_col",
                "fixtime",
                "junk_status",
                "geometry",
                "extra__subject__name",
                "extra__subject__hex",
                "extra__subject__sex",
                "extra__created_at",
                "extra__subject__subject_subtype",
                "extra__subjectsource__id",
                "extra__subjectsource__assigned_range",
                "extra__observation_details",
            ],
            filter_point_coords=[
                {"x": 180.0, "y": 90.0},
                {"x": 0.0, "y": 0.0},
                {"x": 1.0, "y": 1.0},
            ],
            **(params_dict.get("subject_reloc") or {}),
        )
        .call()
    )

    get_custom_previous_period = (
        get_previous_period.validate()
        .set_task_instance_id("get_custom_previous_period")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            time_range=time_range,
            **(params_dict.get("get_custom_previous_period") or {}),
        )
        .call()
    )

    previous_subject_observations = (
        get_subjectgroup_observations.validate()
        .set_task_instance_id("previous_subject_observations")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            filter="clean",
            client=er_client_name,
            time_range=get_custom_previous_period,
            subject_group_name=subject_group_var,
            raise_on_empty=False,
            include_details=True,
            include_subjectsource_details=True,
            **(params_dict.get("previous_subject_observations") or {}),
        )
        .call()
    )

    previous_subject_reloc = (
        process_relocations.validate()
        .set_task_instance_id("previous_subject_reloc")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            observations=previous_subject_observations,
            relocs_columns=[
                "groupby_col",
                "fixtime",
                "junk_status",
                "geometry",
                "extra__subject__name",
                "extra__subject__hex",
                "extra__subject__sex",
                "extra__created_at",
                "extra__subject__subject_subtype",
                "extra__subjectsource__id",
                "extra__subjectsource__assigned_range",
                "extra__observation_details",
            ],
            filter_point_coords=[
                {"x": 180.0, "y": 90.0},
                {"x": 0.0, "y": 0.0},
                {"x": 1.0, "y": 1.0},
            ],
            **(params_dict.get("previous_subject_reloc") or {}),
        )
        .call()
    )

    process_subject_charts = (
        process_collar_voltage_charts.validate()
        .set_task_instance_id("process_subject_charts")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            relocs=subject_observations,
            previous_relocs=previous_subject_observations,
            time_range=time_range,
            output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            **(params_dict.get("process_subject_charts") or {}),
        )
        .call()
    )

    convert_to_trajectories = (
        relocations_to_trajectory.validate()
        .set_task_instance_id("convert_to_trajectories")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            relocations=subject_reloc,
            trajectory_segment_filter={
                "min_length_meters": 0.001,
                "max_length_meters": 5000,
                "min_time_secs": 1,
                "max_time_secs": 21600,
                "min_speed_kmhr": 0.01,
                "max_speed_kmhr": 9,
            },
            **(params_dict.get("convert_to_trajectories") or {}),
        )
        .call()
    )

    add_temporal_index_to_traj = (
        add_temporal_index.validate()
        .set_task_instance_id("add_temporal_index_to_traj")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=convert_to_trajectories,
            time_col="segment_start",
            groupers=groupers,
            cast_to_datetime=True,
            format="mixed",
            **(params_dict.get("add_temporal_index_to_traj") or {}),
        )
        .call()
    )

    classify_trajectories_speed_bins = (
        apply_classification.validate()
        .set_task_instance_id("classify_trajectories_speed_bins")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=add_temporal_index_to_traj,
            input_column_name="speed_kmhr",
            output_column_name="speed_bins",
            classification_options={"scheme": "equal_interval", "k": 6},
            label_options={
                "label_ranges": True,
                "label_decimals": 1,
                "label_suffix": " km/h",
            },
            **(params_dict.get("classify_trajectories_speed_bins") or {}),
        )
        .call()
    )

    sort_trajs_by_speed = (
        sort_values.validate()
        .set_task_instance_id("sort_trajs_by_speed")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            column_name="speed_bins",
            na_position="first",
            ascending=True,
            df=classify_trajectories_speed_bins,
            **(params_dict.get("sort_trajs_by_speed") or {}),
        )
        .call()
    )

    apply_speed_colormap = (
        apply_color_map.validate()
        .set_task_instance_id("apply_speed_colormap")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            input_column_name="speed_bins",
            output_column_name="speed_bins_colormap",
            colormap=["#1a9850", "#91cf60", "#d9ef8b", "#fee08b", "#fc8d59", "#d73027"],
            df=sort_trajs_by_speed,
            **(params_dict.get("apply_speed_colormap") or {}),
        )
        .call()
    )

    filter_speed_cols = (
        filter_df_cols.validate()
        .set_task_instance_id("filter_speed_cols")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            columns=[
                "dist_meters",
                "speed_bins_colormap",
                "geometry",
                "speed_kmhr",
                "speed_bins",
            ],
            df=apply_speed_colormap,
            **(params_dict.get("filter_speed_cols") or {}),
        )
        .call()
    )

    generate_speedmap_layers = (
        create_path_layer.validate()
        .set_task_instance_id("generate_speedmap_layers")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            layer_style={
                "get_color": "speed_bins_colormap",
                "get_width": 2.85,
                "width_scale": 1,
                "width_min_pixels": 2,
                "width_max_pixels": 8,
                "width_units": "pixels",
                "cap_rounded": True,
                "joint_rounded": True,
                "billboard": False,
                "opacity": 0.55,
                "stroked": True,
            },
            legend={
                "title": "Speed (km/h)",
                "label_column": "speed_bins",
                "color_column": "speed_bins_colormap",
                "sort": "ascending",
                "label_suffix": None,
            },
            geodataframe=filter_speed_cols,
            **(params_dict.get("generate_speedmap_layers") or {}),
        )
        .call()
    )

    zoom_speed_gdf_extent = (
        view_state_deck_gdf.validate()
        .set_task_instance_id("zoom_speed_gdf_extent")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            pitch=0,
            bearing=0,
            gdf=filter_speed_cols,
            **(params_dict.get("zoom_speed_gdf_extent") or {}),
        )
        .call()
    )

    draw_speedmap = (
        draw_map.validate()
        .set_task_instance_id("draw_speedmap")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            tile_layers=configure_base_maps,
            static=False,
            title=None,
            max_zoom=10,
            legend_style={"placement": "bottom-right"},
            geo_layers=generate_speedmap_layers,
            view_state=zoom_speed_gdf_extent,
            **(params_dict.get("draw_speedmap") or {}),
        )
        .call()
    )

    persist_speedmap_html = (
        persist_text.validate()
        .set_task_instance_id("persist_speedmap_html")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            filename_suffix="speedmap",
            text=draw_speedmap,
            **(params_dict.get("persist_speedmap_html") or {}),
        )
        .call()
    )

    get_sitrep_config = (
        get_sitrep_event_config.validate()
        .set_task_instance_id("get_sitrep_config")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(region_column="region", **(params_dict.get("get_sitrep_config") or {}))
        .call()
    )

    generate_sitrep_df = (
        compile_sitrep.validate()
        .set_task_instance_id("generate_sitrep_df")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            er_io=er_client_name,
            event_details=get_sitrep_config,
            time_range=time_range,
            **(params_dict.get("generate_sitrep_df") or {}),
        )
        .call()
    )

    persist_sitrep_csv = (
        persist_df.validate()
        .set_task_instance_id("persist_sitrep_csv")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            filetype="csv",
            filename="sitrep_report",
            df=generate_sitrep_df,
            **(params_dict.get("persist_sitrep_csv") or {}),
        )
        .call()
    )

    vehicle_patrols = (
        get_patrol_observations.validate()
        .set_task_instance_id("vehicle_patrols")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            client=er_client_name,
            time_range=time_range,
            include_patrol_details=True,
            raise_on_empty=True,
            sub_page_size=100,
            patrol_types=[
                "MEP_routine_vehicle_patrol_bravo_team",
                "MEP_routine_vehicle_patrol_foxtrot_team",
                "MEP_Routine_Vehicle_Patrol - Mwaluganje Team",
                "MEP_routine_vehicle_patrol_hq_team",
                "MEP_routine_vehicle_patrol_delta_team",
                "MEP_routine_vehicle_patrol_echo_team",
                "MEP_routine_vehicle_patrol_kilo_team",
                "MEP_routine_vehicle_patrol_mobile_team",
                "MEP_routine_vehicle_patrol_alpha_team",
                "MEP_routine_vehicle_patrol_charlie_team",
                "MEP_routine_vehicle_patrol_golf_team",
            ],
            **(params_dict.get("vehicle_patrols") or {}),
        )
        .call()
    )

    vehicle_patrol_reloc = (
        process_relocations.validate()
        .set_task_instance_id("vehicle_patrol_reloc")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            observations=vehicle_patrols,
            relocs_columns=[
                "patrol_id",
                "patrol_start_time",
                "patrol_end_time",
                "geometry",
                "patrol_type__value",
                "patrol_type__display",
                "patrol_serial_number",
                "patrol_status",
                "patrol_subject",
                "groupby_col",
                "fixtime",
                "junk_status",
                "extra__source",
            ],
            filter_point_coords=[
                {"x": 180.0, "y": 90.0},
                {"x": 0.0, "y": 0.0},
                {"x": 1.0, "y": 1.0},
            ],
            **(params_dict.get("vehicle_patrol_reloc") or {}),
        )
        .call()
    )

    vehicle_patrol_traj = (
        relocations_to_trajectory.validate()
        .set_task_instance_id("vehicle_patrol_traj")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            relocations=vehicle_patrol_reloc,
            trajectory_segment_filter={
                "min_length_meters": 0.35,
                "max_length_meters": 5000.0,
                "max_time_secs": 18000.0,
                "min_time_secs": 1.0,
                "max_speed_kmhr": 100.0,
                "min_speed_kmhr": 10.0,
            },
            **(params_dict.get("vehicle_patrol_traj") or {}),
        )
        .call()
    )

    persist_vehicle_traj = (
        persist_df.validate()
        .set_task_instance_id("persist_vehicle_traj")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=vehicle_patrol_traj,
            filetype="geoparquet",
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            filename="vehicle_patrol_trajectories",
            **(params_dict.get("persist_vehicle_traj") or {}),
        )
        .call()
    )

    vehicle_traj_colormap = (
        apply_color_map.validate()
        .set_task_instance_id("vehicle_traj_colormap")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=vehicle_patrol_traj,
            colormap="viridis",
            input_column_name="extra__patrol_type__value",
            output_column_name="patrol_type_colormap",
            **(params_dict.get("vehicle_traj_colormap") or {}),
        )
        .call()
    )

    vehicle_zoom_value = (
        view_state_deck_gdf.validate()
        .set_task_instance_id("vehicle_zoom_value")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            pitch=0,
            bearing=0,
            gdf=vehicle_traj_colormap,
            **(params_dict.get("vehicle_zoom_value") or {}),
        )
        .call()
    )

    vehicle_patrol_path_layer = (
        create_path_layer.validate()
        .set_task_instance_id("vehicle_patrol_path_layer")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            layer_style={
                "get_color": "patrol_type_colormap",
                "get_width": 2.85,
                "width_scale": 1,
                "width_min_pixels": 2,
                "width_max_pixels": 8,
                "width_units": "pixels",
                "cap_rounded": True,
                "joint_rounded": True,
                "billboard": False,
                "opacity": 0.55,
                "stroked": True,
            },
            legend={
                "title": "Patrol team",
                "label_column": "extra__patrol_type__value",
                "color_column": "patrol_type_colormap",
                "sort": "ascending",
                "label_suffix": None,
            },
            geodataframe=vehicle_traj_colormap,
            **(params_dict.get("vehicle_patrol_path_layer") or {}),
        )
        .call()
    )

    draw_vehicles_map = (
        draw_map.validate()
        .set_task_instance_id("draw_vehicles_map")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            tile_layers=configure_base_maps,
            static=False,
            title=None,
            max_zoom=10,
            legend_style={"placement": "bottom-right"},
            geo_layers=vehicle_patrol_path_layer,
            view_state=vehicle_zoom_value,
            **(params_dict.get("draw_vehicles_map") or {}),
        )
        .call()
    )

    vehicle_patrol_map = (
        persist_text.validate()
        .set_task_instance_id("vehicle_patrol_map")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            filename="vehicle_patrols_map.html",
            text=draw_vehicles_map,
            **(params_dict.get("vehicle_patrol_map") or {}),
        )
        .call()
    )

    foot_patrols = (
        get_patrol_observations.validate()
        .set_task_instance_id("foot_patrols")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            client=er_client_name,
            time_range=time_range,
            include_patrol_details=True,
            raise_on_empty=True,
            sub_page_size=100,
            patrol_types=[
                "MEP_routine_foot_patrol_bravo_team",
                "MEP_routine_foot_patrol_foxtrot_team",
                "MEP_routine_foot_patrol_HQ_team",
                "MEP_routine_foot_patrol_ Delta_Team",
                "MEP_routine_foot_patrol_echo_team",
                "MEP_routine_foot_patrol_kilo_team",
                "mwaluganje_routine_foot_patrol_alpha_team",
                "MEP_routine_foot_patrol_alpha_team",
                "MEP_routine_foot_patrol_charlie_team",
                "MEP_routine_foot_patrol_golf_team",
                "MEP_routine_foot_patrol_marmanet",
            ],
            **(params_dict.get("foot_patrols") or {}),
        )
        .call()
    )

    foot_patrol_reloc = (
        process_relocations.validate()
        .set_task_instance_id("foot_patrol_reloc")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            observations=foot_patrols,
            relocs_columns=[
                "patrol_id",
                "patrol_start_time",
                "patrol_end_time",
                "geometry",
                "patrol_type__value",
                "patrol_type__display",
                "patrol_serial_number",
                "patrol_status",
                "patrol_subject",
                "groupby_col",
                "fixtime",
                "junk_status",
                "extra__source",
            ],
            filter_point_coords=[
                {"x": 180.0, "y": 90.0},
                {"x": 0.0, "y": 0.0},
                {"x": 1.0, "y": 1.0},
            ],
            **(params_dict.get("foot_patrol_reloc") or {}),
        )
        .call()
    )

    foot_patrol_traj = (
        relocations_to_trajectory.validate()
        .set_task_instance_id("foot_patrol_traj")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            relocations=foot_patrol_reloc,
            trajectory_segment_filter={
                "min_length_meters": 0.001,
                "max_length_meters": 5000.0,
                "max_time_secs": 14400.0,
                "min_time_secs": 1.0,
                "max_speed_kmhr": 9.0,
                "min_speed_kmhr": 0.5,
            },
            **(params_dict.get("foot_patrol_traj") or {}),
        )
        .call()
    )

    persist_foot_traj = (
        persist_df.validate()
        .set_task_instance_id("persist_foot_traj")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=foot_patrol_traj,
            filetype="geoparquet",
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            filename="foot_patrol_trajectories",
            **(params_dict.get("persist_foot_traj") or {}),
        )
        .call()
    )

    foot_traj_colormap = (
        apply_color_map.validate()
        .set_task_instance_id("foot_traj_colormap")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=foot_patrol_traj,
            colormap="viridis",
            input_column_name="extra__patrol_type__value",
            output_column_name="patrol_type_colormap",
            **(params_dict.get("foot_traj_colormap") or {}),
        )
        .call()
    )

    foot_zoom_value = (
        view_state_deck_gdf.validate()
        .set_task_instance_id("foot_zoom_value")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            pitch=0,
            bearing=0,
            gdf=foot_traj_colormap,
            **(params_dict.get("foot_zoom_value") or {}),
        )
        .call()
    )

    foot_patrol_path_layer = (
        create_path_layer.validate()
        .set_task_instance_id("foot_patrol_path_layer")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            layer_style={
                "get_color": "patrol_type_colormap",
                "get_width": 2.85,
                "width_scale": 1,
                "width_min_pixels": 2,
                "width_max_pixels": 8,
                "width_units": "pixels",
                "cap_rounded": True,
                "joint_rounded": True,
                "billboard": False,
                "opacity": 0.55,
                "stroked": True,
            },
            legend={
                "title": "Patrol team",
                "label_column": "extra__patrol_type__value",
                "color_column": "patrol_type_colormap",
                "sort": "ascending",
                "label_suffix": None,
            },
            geodataframe=foot_traj_colormap,
            **(params_dict.get("foot_patrol_path_layer") or {}),
        )
        .call()
    )

    draw_foot_map = (
        draw_map.validate()
        .set_task_instance_id("draw_foot_map")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            tile_layers=configure_base_maps,
            static=False,
            title=None,
            max_zoom=10,
            legend_style={"placement": "bottom-right"},
            geo_layers=foot_patrol_path_layer,
            view_state=foot_zoom_value,
            **(params_dict.get("draw_foot_map") or {}),
        )
        .call()
    )

    foot_patrol_map = (
        persist_text.validate()
        .set_task_instance_id("foot_patrol_map")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            filename="foot_patrols_map.html",
            text=draw_foot_map,
            **(params_dict.get("foot_patrol_map") or {}),
        )
        .call()
    )

    download_roi_file = (
        fetch_and_persist_file.validate()
        .set_task_instance_id("download_roi_file")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            url="https://www.dropbox.com/scl/fi/2bpktq45zns9igryl6q9l/ROIs.gpkg?rlkey=sojch2njmvsa3i5a3f3pt11xq&st=9x70z6z1&dl=0",
            output_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            overwrite_existing=False,
            retries=3,
            unzip=False,
            **(params_dict.get("download_roi_file") or {}),
        )
        .call()
    )

    load_roi = (
        load_df.validate()
        .set_task_instance_id("load_roi")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            file_path=download_roi_file,
            layer=None,
            deserialize_json=False,
            **(params_dict.get("load_roi") or {}),
        )
        .call()
    )

    transform_roi = (
        transform_gdf_crs.validate()
        .set_task_instance_id("transform_roi")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            gdf=load_roi, crs="EPSG:4326", **(params_dict.get("transform_roi") or {})
        )
        .call()
    )

    process_ndvi_charts = (
        process_aoi_ndvi_charts.validate()
        .set_task_instance_id("process_ndvi_charts")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=transform_roi,
            er_client=gee_project_name,
            aoi_column="name",
            time_range=time_range,
            output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            **(params_dict.get("process_ndvi_charts") or {}),
        )
        .call()
    )

    convert_sightings_png = (
        html_to_png.validate()
        .set_task_instance_id("convert_sightings_png")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            html_path=persist_sightings_urls,
            config={
                "full_page": False,
                "device_scale_factor": 2.0,
                "wait_for_timeout": 40000,
                "max_concurrent_pages": 1,
            },
            **(params_dict.get("convert_sightings_png") or {}),
        )
        .call()
    )

    convert_speedmap_png = (
        html_to_png.validate()
        .set_task_instance_id("convert_speedmap_png")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            html_path=persist_speedmap_html,
            config={
                "full_page": False,
                "device_scale_factor": 2.0,
                "wait_for_timeout": 40000,
                "max_concurrent_pages": 1,
            },
            **(params_dict.get("convert_speedmap_png") or {}),
        )
        .call()
    )

    convert_vehicle_png = (
        html_to_png.validate()
        .set_task_instance_id("convert_vehicle_png")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            html_path=vehicle_patrol_map,
            config={
                "full_page": False,
                "device_scale_factor": 2.0,
                "wait_for_timeout": 40000,
                "max_concurrent_pages": 1,
            },
            **(params_dict.get("convert_vehicle_png") or {}),
        )
        .call()
    )

    convert_foot_png = (
        html_to_png.validate()
        .set_task_instance_id("convert_foot_png")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            html_path=foot_patrol_map,
            config={
                "full_page": False,
                "device_scale_factor": 2.0,
                "wait_for_timeout": 40000,
                "max_concurrent_pages": 1,
            },
            **(params_dict.get("convert_foot_png") or {}),
        )
        .call()
    )

    convert_ndvi_png = (
        html_to_png.validate()
        .set_task_instance_id("convert_ndvi_png")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            html_path=process_ndvi_charts,
            config={
                "full_page": False,
                "device_scale_factor": 2.0,
                "wait_for_timeout": 10,
                "max_concurrent_pages": 1,
            },
            **(params_dict.get("convert_ndvi_png") or {}),
        )
        .call()
    )

    convert_collared_png = (
        html_to_png.validate()
        .set_task_instance_id("convert_collared_png")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            html_path=process_subject_charts,
            config={
                "full_page": False,
                "device_scale_factor": 2.0,
                "wait_for_timeout": 10,
                "max_concurrent_pages": 1,
            },
            **(params_dict.get("convert_collared_png") or {}),
        )
        .call()
    )

    download_cover_page = (
        fetch_and_persist_file.validate()
        .set_task_instance_id("download_cover_page")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            url="https://www.dropbox.com/scl/fi/95fux8w00h9u4wg2sufij/mep_monthly_report.docx?rlkey=nbibg8ulnlz0w4q53jw2db6y3&st=6own6h01&dl=0",
            output_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            overwrite_existing=False,
            retries=3,
            unzip=False,
            **(params_dict.get("download_cover_page") or {}),
        )
        .call()
    )

    download_content_page = (
        fetch_and_persist_file.validate()
        .set_task_instance_id("download_content_page")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            url="https://www.dropbox.com/scl/fi/1u7d68pr8hvc27gf2ns85/mep_monthly_indv_report.docx?rlkey=wss0x8sa9i5fgl9yjco7paa03&st=b8fsdnxg&dl=0",
            output_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            overwrite_existing=False,
            retries=3,
            unzip=False,
            **(params_dict.get("download_content_page") or {}),
        )
        .call()
    )

    create_cover_tpl_context = (
        create_monthly_ctx_cover.validate()
        .set_task_instance_id("create_cover_tpl_context")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            report_period=time_range,
            prepared_by="Ecoscope",
            **(params_dict.get("create_cover_tpl_context") or {}),
        )
        .call()
    )

    persist_cover_context = (
        create__mep_context_page.validate()
        .set_task_instance_id("persist_cover_context")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            template_path=download_cover_page,
            output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            context=create_cover_tpl_context,
            filename="mep_cover_page.docx",
            **(params_dict.get("persist_cover_context") or {}),
        )
        .call()
    )

    create_monthly_ctx = (
        create_mep_monthly_context.validate()
        .set_task_instance_id("create_monthly_ctx")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            filename="mep_context.docx",
            template_path=download_content_page,
            output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            elephant_sightings_map_path=convert_sightings_png,
            speedmap_path=convert_speedmap_png,
            foot_patrols_map_path=convert_foot_png,
            vehicle_patrol_map_path=convert_vehicle_png,
            collared_elephant_plot_paths=convert_collared_png,
            regional_ndvi_plot_paths=convert_ndvi_png,
            sitrep_df_path=persist_sitrep_csv,
            **(params_dict.get("create_monthly_ctx") or {}),
        )
        .call()
    )

    merge_mep_docx = (
        merge_mapbook_files.validate()
        .set_task_instance_id("merge_mep_docx")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            cover_page_path=persist_cover_context,
            output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            context_page_items=[create_monthly_ctx],
            filename=None,
            **(params_dict.get("merge_mep_docx") or {}),
        )
        .call()
    )

    mep_monthly_dashboard = (
        gather_dashboard.validate()
        .set_task_instance_id("mep_monthly_dashboard")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            details=workflow_details,
            widgets=[],
            time_range=time_range,
            groupers=groupers,
            **(params_dict.get("mep_monthly_dashboard") or {}),
        )
        .call()
    )

    return mep_monthly_dashboard
