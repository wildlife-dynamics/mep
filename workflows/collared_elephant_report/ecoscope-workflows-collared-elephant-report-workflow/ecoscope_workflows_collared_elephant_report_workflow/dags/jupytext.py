# AUTOGENERATED BY ECOSCOPE-WORKFLOWS; see fingerprint in README.md for details


# ruff: noqa: E402

# %% [markdown]
# # Collared Elephant Report
# TODO: top level description

# %% [markdown]
# ## Imports

import os
from ecoscope_workflows_core.tasks.config import set_workflow_details
from ecoscope_workflows_core.tasks.io import set_gee_connection
from ecoscope_workflows_core.tasks.io import set_er_connection
from ecoscope_workflows_ext_ecoscope.tasks.results import set_base_maps
from ecoscope_workflows_core.tasks.groupby import set_groupers
from ecoscope_workflows_core.tasks.filter import set_time_range
from ecoscope_workflows_ext_mep.tasks import download_file_and_persist
from ecoscope_workflows_ext_mep.tasks import load_landdx_aoi
from ecoscope_workflows_ext_mep.tasks import split_gdf_by_column
from ecoscope_workflows_ext_mep.tasks import annotate_gdf_dict_with_geometry_type
from ecoscope_workflows_ext_mep.tasks import create_map_layers_from_annotated_dict
from ecoscope_workflows_ext_mep.tasks import get_subjects_info
from ecoscope_workflows_ext_ecoscope.tasks.transformation import normalize_column
from ecoscope_workflows_core.tasks.transformation import map_columns
from ecoscope_workflows_ext_ecoscope.tasks.io import get_subjectgroup_observations
from ecoscope_workflows_ext_ecoscope.tasks.preprocessing import process_relocations
from ecoscope_workflows_ext_mep.tasks import view_df
from ecoscope_workflows_ext_mep.tasks import compute_maturity
from ecoscope_workflows_core.tasks.groupby import split_groups
from ecoscope_workflows_ext_mep.tasks import download_profile_photo
from ecoscope_workflows_ext_mep.tasks import generate_subject_info
from ecoscope_workflows_ext_ecoscope.tasks.io import persist_df
from ecoscope_workflows_ext_ecoscope.tasks.io import get_events
from ecoscope_workflows_core.tasks.skip import any_is_empty_df
from ecoscope_workflows_core.tasks.skip import any_dependency_skipped
from ecoscope_workflows_ext_ecoscope.tasks.preprocessing import (
    relocations_to_trajectory,
)
from ecoscope_workflows_core.tasks.transformation import add_temporal_index
from ecoscope_workflows_ext_ecoscope.tasks.transformation import apply_classification
from ecoscope_workflows_core.tasks.transformation import sort_values
from ecoscope_workflows_ext_ecoscope.tasks.transformation import apply_color_map
from ecoscope_workflows_core.tasks.transformation import map_values_with_unit
from ecoscope_workflows_ext_ecoscope.tasks.results import create_polyline_layer
from ecoscope_workflows_ext_mep.tasks import create_view_state_from_gdf
from ecoscope_workflows_ext_mep.tasks import combine_map_layers
from ecoscope_workflows_ext_mep.tasks import zip_grouped_by_key
from ecoscope_workflows_ext_ecoscope.tasks.results import draw_ecomap
from ecoscope_workflows_core.tasks.io import persist_text
from ecoscope_workflows_core.tasks.results import create_map_widget_single_view
from ecoscope_workflows_core.tasks.skip import never
from ecoscope_workflows_core.tasks.results import merge_widget_views
from ecoscope_workflows_ext_ecoscope.tasks.analysis import (
    calculate_elliptical_time_density,
)
from ecoscope_workflows_ext_ecoscope.tasks.io import determine_season_windows
from ecoscope_workflows_ext_mep.tasks import create_seasonal_labels
from ecoscope_workflows_ext_mep.tasks import generate_mcp_gdf
from ecoscope_workflows_ext_ecoscope.tasks.results import create_polygon_layer
from ecoscope_workflows_ext_mep.tasks import calculate_seasonal_home_range
from ecoscope_workflows_ext_ecoscope.tasks.skip import all_geometry_are_none
from ecoscope_workflows_ext_mep.tasks import generate_seasonal_nsd_plot
from ecoscope_workflows_ext_mep.tasks import generate_seasonal_speed_plot
from ecoscope_workflows_ext_mep.tasks import generate_collared_seasonal_plot
from ecoscope_workflows_ext_mep.tasks import generate_seasonal_mcp_asymptote_plot
from ecoscope_workflows_ext_mep.tasks import get_subject_stats
from ecoscope_workflows_ext_mep.tasks import build_template_region_lookup
from ecoscope_workflows_ext_mep.tasks import compute_template_regions
from ecoscope_workflows_ext_mep.tasks import compute_subject_occupancy
from ecoscope_workflows_ext_custom.tasks import html_to_png
from ecoscope_workflows_ext_mep.tasks import flatten_tuple
from ecoscope_workflows_ext_mep.tasks import print_output
from ecoscope_workflows_ext_mep.tasks import report_context
from ecoscope_workflows_ext_mep.tasks import render_html_to_pdf
from ecoscope_workflows_ext_mep.tasks import merge_pdfs
from ecoscope_workflows_core.tasks.results import gather_dashboard

# %% [markdown]
# ## Set Workflow Details

# %%
# parameters

workflow_details_params = dict(
    name=...,
    description=...,
    image_url=...,
)

# %%
# call the task


workflow_details = (
    set_workflow_details.handle_errors(task_instance_id="workflow_details")
    .partial(**workflow_details_params)
    .call()
)


# %% [markdown]
# ## Set Google Earth Engine Data Source

# %%
# parameters

gee_client_params = dict(
    data_source=...,
)

# %%
# call the task


gee_client = (
    set_gee_connection.handle_errors(task_instance_id="gee_client")
    .partial(**gee_client_params)
    .call()
)


# %% [markdown]
# ## Connect to Earth Ranger instance

# %%
# parameters

er_client_params = dict(
    data_source=...,
)

# %%
# call the task


er_client = (
    set_er_connection.handle_errors(task_instance_id="er_client")
    .partial(**er_client_params)
    .call()
)


# %% [markdown]
# ## Configure Base Map Layers

# %%
# parameters

configure_base_maps_params = dict(
    base_maps=...,
)

# %%
# call the task


configure_base_maps = (
    set_base_maps.handle_errors(task_instance_id="configure_base_maps")
    .partial(**configure_base_maps_params)
    .call()
)


# %% [markdown]
# ## Configure grouping strategy

# %%
# parameters

configure_grouping_strategy_params = dict(
    groupers=...,
)

# %%
# call the task


configure_grouping_strategy = (
    set_groupers.handle_errors(task_instance_id="configure_grouping_strategy")
    .partial(**configure_grouping_strategy_params)
    .call()
)


# %% [markdown]
# ## Define time range

# %%
# parameters

define_time_range_params = dict(
    since=...,
    until=...,
)

# %%
# call the task


define_time_range = (
    set_time_range.handle_errors(task_instance_id="define_time_range")
    .partial(time_format="%d %b %Y %H:%M:%S %Z", **define_time_range_params)
    .call()
)


# %% [markdown]
# ## Retrieve and unpack landDX db

# %%
# parameters

retrieve_ldx_db_params = dict(
    retries=...,
    overwrite_existing=...,
)

# %%
# call the task


retrieve_ldx_db = (
    download_file_and_persist.handle_errors(task_instance_id="retrieve_ldx_db")
    .partial(
        output_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        url="https://maraelephant.maps.arcgis.com/sharing/rest/content/items/6da0c9bdd43d4dd0ac59a4f3cd73dcab/data",
        unzip=True,
        **retrieve_ldx_db_params,
    )
    .call()
)


# %% [markdown]
# ## Download Logo

# %%
# parameters

download_logo_params = dict(
    url=...,
    retries=...,
    unzip=...,
)

# %%
# call the task


download_logo = (
    download_file_and_persist.handle_errors(task_instance_id="download_logo")
    .partial(
        output_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        overwrite_existing=False,
        **download_logo_params,
    )
    .call()
)


# %% [markdown]
# ## Download report cover page

# %%
# parameters

download_cover_page_params = dict(
    retries=...,
    unzip=...,
)

# %%
# call the task


download_cover_page = (
    download_file_and_persist.handle_errors(task_instance_id="download_cover_page")
    .partial(
        url="https://www.dropbox.com/scl/fi/my6cd3fhs8wtkv34sqb0l/cover_page.pdf?rlkey=45ejxhslkmpa05nhdv6ehxnod&dl=1",
        output_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        overwrite_existing=False,
        **download_cover_page_params,
    )
    .call()
)


# %% [markdown]
# ## Download template one

# %%
# parameters

download_template_one_params = dict(
    retries=...,
    unzip=...,
)

# %%
# call the task


download_template_one = (
    download_file_and_persist.handle_errors(task_instance_id="download_template_one")
    .partial(
        url="https://www.dropbox.com/scl/fi/6yruu5uyino3gmzasdfy0/template_1.html?rlkey=w7haw2q8hkpv1ocsms0zq92rw&st=7bak4xbj&dl=0",
        output_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        overwrite_existing=False,
        **download_template_one_params,
    )
    .call()
)


# %% [markdown]
# ## Download template two

# %%
# parameters

download_template_two_params = dict(
    retries=...,
    unzip=...,
)

# %%
# call the task


download_template_two = (
    download_file_and_persist.handle_errors(task_instance_id="download_template_two")
    .partial(
        url="https://www.dropbox.com/scl/fi/6skjcsspv9rf5jgsyedgt/template_2.html?rlkey=lk73uq9m506ve9qsmghkk57k2&st=6lbck9k3&dl=0",
        output_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        overwrite_existing=False,
        **download_template_two_params,
    )
    .call()
)


# %% [markdown]
# ## Download template three

# %%
# parameters

download_template_three_params = dict(
    retries=...,
    unzip=...,
)

# %%
# call the task


download_template_three = (
    download_file_and_persist.handle_errors(task_instance_id="download_template_three")
    .partial(
        url="https://www.dropbox.com/scl/fi/44yv82b85lws8u657nuoy/template_3.html?rlkey=b0bdy09kb2oom13rvmd6r3gbc&st=550bazn1&dl=0",
        output_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        overwrite_existing=False,
        **download_template_three_params,
    )
    .call()
)


# %% [markdown]
# ## Load AOI from landDX

# %%
# parameters

load_aoi_params = dict(
    aoi=...,
)

# %%
# call the task


load_aoi = (
    load_landdx_aoi.handle_errors(task_instance_id="load_aoi")
    .partial(map_path=retrieve_ldx_db, **load_aoi_params)
    .call()
)


# %% [markdown]
# ## split landDX layers by type

# %%
# parameters

split_landdx_by_type_params = dict()

# %%
# call the task


split_landdx_by_type = (
    split_gdf_by_column.handle_errors(task_instance_id="split_landdx_by_type")
    .partial(gdf=load_aoi, column="type", **split_landdx_by_type_params)
    .call()
)


# %% [markdown]
# ## Annotate geometry types

# %%
# parameters

annotate_geom_types_params = dict()

# %%
# call the task


annotate_geom_types = (
    annotate_gdf_dict_with_geometry_type.handle_errors(
        task_instance_id="annotate_geom_types"
    )
    .partial(gdf_dict=split_landdx_by_type, **annotate_geom_types_params)
    .call()
)


# %% [markdown]
# ## Style landDX map layers

# %%
# parameters

create_styled_ldx_layers_params = dict(
    style_config=...,
)

# %%
# call the task


create_styled_ldx_layers = (
    create_map_layers_from_annotated_dict.handle_errors(
        task_instance_id="create_styled_ldx_layers"
    )
    .partial(annotated_dict=annotate_geom_types, **create_styled_ldx_layers_params)
    .call()
)


# %% [markdown]
# ## Get subject dataframe

# %%
# parameters

subject_df_params = dict(
    bbox=...,
    subject_group_id=...,
    subject_group_name=...,
    name=...,
    updated_since=...,
    updated_until=...,
    tracks=...,
    ids=...,
    max_ids_per_request=...,
    raise_on_empty=...,
)

# %%
# call the task


subject_df = (
    get_subjects_info.handle_errors(task_instance_id="subject_df")
    .partial(client=er_client, include_inactive=True, **subject_df_params)
    .call()
)


# %% [markdown]
# ## Normalize additional subject info

# %%
# parameters

normalize_subject_info_params = dict()

# %%
# call the task


normalize_subject_info = (
    normalize_column.handle_errors(task_instance_id="normalize_subject_info")
    .partial(column="additional", df=subject_df, **normalize_subject_info_params)
    .call()
)


# %% [markdown]
# ## Rename subject columns

# %%
# parameters

rename_subject_cols_params = dict()

# %%
# call the task


rename_subject_cols = (
    map_columns.handle_errors(task_instance_id="rename_subject_cols")
    .partial(
        drop_columns=[
            "url",
            "image_url",
            "common_name",
            "content_type",
            "additional__external_id",
            "additional__tm_animal_id",
            "additional__external_name",
        ],
        retain_columns=[
            "id",
            "name",
            "hex",
            "additional__rgb",
            "additional__sex",
            "additional__Bio",
            "additional__DOB",
            "additional__notes",
            "additional__status",
            "additional__region",
            "additional__country",
            "additional__id_photo",
            "additional__distribution",
        ],
        rename_columns={
            "id": "groupby_col",
            "name": "subject_name",
            "hex": "hex_color",
            "additional__rgb": "rgb",
            "additional__Bio": "subject_bio",
            "additional__sex": "subject_sex",
            "additional__DOB": "date_of_birth",
            "additional__notes": "notes",
            "additional__status": "status",
            "additional__region": "region",
            "additional__country": "country",
            "additional__id_photo": "photo",
            "additional__distribution": "distribution",
        },
        df=normalize_subject_info,
        **rename_subject_cols_params,
    )
    .call()
)


# %% [markdown]
# ## Get subject group observations from ER

# %%
# parameters

subject_observations_params = dict(
    subject_group_name=...,
)

# %%
# call the task


subject_observations = (
    get_subjectgroup_observations.handle_errors(task_instance_id="subject_observations")
    .partial(
        client=er_client,
        time_range=define_time_range,
        raise_on_empty=False,
        include_details=False,
        include_subjectsource_details=False,
        **subject_observations_params,
    )
    .call()
)


# %% [markdown]
# ## Transform observations to relocations

# %%
# parameters

subject_relocations_params = dict()

# %%
# call the task


subject_relocations = (
    process_relocations.handle_errors(task_instance_id="subject_relocations")
    .partial(
        observations=subject_observations,
        relocs_columns=[
            "fixtime",
            "geometry",
            "groupby_col",
            "junk_status",
            "extra__created_at",
            "extra__subject__sex",
            "extra__subject__hex",
            "extra__subject__name",
            "extra__subject__subject_subtype",
        ],
        filter_point_coords=[
            {"x": 180.0, "y": 90.0},
            {"x": 0.0, "y": 0.0},
            {"x": 1.0, "y": 1.0},
        ],
        **subject_relocations_params,
    )
    .call()
)


# %% [markdown]
# ## View dataframe

# %%
# parameters

inspect_relocations_df_params = dict()

# %%
# call the task


inspect_relocations_df = (
    view_df.handle_errors(task_instance_id="inspect_relocations_df")
    .partial(gdf=subject_relocations, name="Relocs", **inspect_relocations_df_params)
    .call()
)


# %% [markdown]
# ## Rename reloc columns

# %%
# parameters

rename_reloc_cols_params = dict()

# %%
# call the task


rename_reloc_cols = (
    map_columns.handle_errors(task_instance_id="rename_reloc_cols")
    .partial(
        drop_columns=[],
        retain_columns=[],
        rename_columns={
            "name": "subject_name",
            "hex": "hex_color",
            "sex": "subject_sex",
            "subject_subtype": "subject_subtype",
            "created_at": "created_at",
        },
        df=subject_relocations,
        **rename_reloc_cols_params,
    )
    .call()
)


# %% [markdown]
# ## Compute subject maturity

# %%
# parameters

compute_subject_maturity_params = dict(
    time_column=...,
)

# %%
# call the task


compute_subject_maturity = (
    compute_maturity.handle_errors(task_instance_id="compute_subject_maturity")
    .partial(
        subject_df=rename_subject_cols,
        relocations_gdf=rename_reloc_cols,
        months_duration=6,
        **compute_subject_maturity_params,
    )
    .call()
)


# %% [markdown]
# ## split subjects df by group

# %%
# parameters

split_subject_by_group_params = dict()

# %%
# call the task


split_subject_by_group = (
    split_groups.handle_errors(task_instance_id="split_subject_by_group")
    .partial(
        df=compute_subject_maturity,
        groupers=configure_grouping_strategy,
        **split_subject_by_group_params,
    )
    .call()
)


# %% [markdown]
# ## Download subject profile photo

# %%
# parameters

download_profile_pic_params = dict()

# %%
# call the task


download_profile_pic = (
    download_profile_photo.handle_errors(task_instance_id="download_profile_pic")
    .partial(
        image_type=".png",
        column="photo",
        overwrite_existing=True,
        output_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        **download_profile_pic_params,
    )
    .mapvalues(argnames=["df"], argvalues=split_subject_by_group)
)


# %% [markdown]
# ## Download subject information

# %%
# parameters

download_subject_info_params = dict(
    return_data=...,
)

# %%
# call the task


download_subject_info = (
    generate_subject_info.handle_errors(task_instance_id="download_subject_info")
    .partial(
        maxlen=1000,
        output_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        **download_subject_info_params,
    )
    .mapvalues(argnames=["df"], argvalues=split_subject_by_group)
)


# %% [markdown]
# ## Persist subject information

# %%
# parameters

persist_subject_info_params = dict(
    filename=...,
    filetype=...,
)

# %%
# call the task


persist_subject_info = (
    persist_df.handle_errors(task_instance_id="persist_subject_info")
    .partial(
        root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        **persist_subject_info_params,
    )
    .mapvalues(argnames=["df"], argvalues=download_subject_info)
)


# %% [markdown]
# ## Retrieve events from ER

# %%
# parameters

get_events_data_params = dict(
    event_types=...,
    include_null_geometry=...,
    include_updates=...,
    include_related_events=...,
)

# %%
# call the task


get_events_data = (
    get_events.handle_errors(task_instance_id="get_events_data")
    .partial(
        client=er_client,
        time_range=define_time_range,
        include_details=True,
        raise_on_empty=True,
        event_columns=[
            "id",
            "time",
            "event_type",
            "event_category",
            "reported_by",
            "serial_number",
            "geometry",
            "event_details",
        ],
        **get_events_data_params,
    )
    .call()
)


# %% [markdown]
# ## Normalize event details

# %%
# parameters

normalize_event_details_params = dict()

# %%
# call the task


normalize_event_details = (
    normalize_column.handle_errors(task_instance_id="normalize_event_details")
    .partial(
        column="event_details", df=get_events_data, **normalize_event_details_params
    )
    .call()
)


# %% [markdown]
# ## Rename event columns

# %%
# parameters

rename_event_cols_params = dict()

# %%
# call the task


rename_event_cols = (
    map_columns.handle_errors(task_instance_id="rename_event_cols")
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        drop_columns=[],
        retain_columns=[],
        rename_columns={
            "event_details__pic": "pic",
            "event_details__region": "region",
            "event_details__source": "source",
            "event_details__details": "details",
            "event_details__subject": "groupby_col",
            "event_details__source_id": "source_id",
            "event_details__subject_id": "subject_name",
            "event_details__collaring_type": "collaring_type",
            "event_details__collaring_reason": "collaring_reason",
            "event_details__collar_checked_by": "collar_checked_by",
        },
        df=normalize_event_details,
        **rename_event_cols_params,
    )
    .call()
)


# %% [markdown]
# ## split events by group

# %%
# parameters

split_events_by_group_params = dict()

# %%
# call the task


split_events_by_group = (
    split_groups.handle_errors(task_instance_id="split_events_by_group")
    .partial(
        df=rename_event_cols,
        groupers=configure_grouping_strategy,
        **split_events_by_group_params,
    )
    .call()
)


# %% [markdown]
# ## Split relocations by group

# %%
# parameters

split_relocs_by_group_params = dict()

# %%
# call the task


split_relocs_by_group = (
    split_groups.handle_errors(task_instance_id="split_relocs_by_group")
    .partial(
        df=rename_reloc_cols,
        groupers=configure_grouping_strategy,
        **split_relocs_by_group_params,
    )
    .call()
)


# %% [markdown]
# ## Convert relocations to trajectories

# %%
# parameters

convert_to_trajectories_params = dict(
    trajectory_segment_filter=...,
)

# %%
# call the task


convert_to_trajectories = (
    relocations_to_trajectory.handle_errors(task_instance_id="convert_to_trajectories")
    .partial(relocations=rename_reloc_cols, **convert_to_trajectories_params)
    .call()
)


# %% [markdown]
# ## Add temporal index to trajectories

# %%
# parameters

add_temporal_index_to_traj_params = dict()

# %%
# call the task


add_temporal_index_to_traj = (
    add_temporal_index.handle_errors(task_instance_id="add_temporal_index_to_traj")
    .partial(
        df=convert_to_trajectories,
        time_col="segment_start",
        groupers=configure_grouping_strategy,
        cast_to_datetime=True,
        format="mixed",
        **add_temporal_index_to_traj_params,
    )
    .call()
)


# %% [markdown]
# ## Classify trajectories by speed

# %%
# parameters

classify_trajectory_speed_bins_params = dict()

# %%
# call the task


classify_trajectory_speed_bins = (
    apply_classification.handle_errors(
        task_instance_id="classify_trajectory_speed_bins"
    )
    .partial(
        df=add_temporal_index_to_traj,
        input_column_name="speed_kmhr",
        output_column_name="speed_bins",
        classification_options={"scheme": "equal_interval", "k": 6},
        label_options={"label_ranges": False, "label_decimals": 1},
        **classify_trajectory_speed_bins_params,
    )
    .call()
)


# %% [markdown]
# ## Rename trajectory columns

# %%
# parameters

rename_trajectory_cols_params = dict()

# %%
# call the task


rename_trajectory_cols = (
    map_columns.handle_errors(task_instance_id="rename_trajectory_cols")
    .partial(
        drop_columns=[],
        retain_columns=[],
        rename_columns={
            "extra__subject_name": "subject_name",
            "extra__hex_color": "hex_color",
            "extra__subject_sex": "subject_sex",
            "extra__subject_subtype": "subject_subtype",
        },
        df=classify_trajectory_speed_bins,
        **rename_trajectory_cols_params,
    )
    .call()
)


# %% [markdown]
# ## Split trajectories by group

# %%
# parameters

split_trajs_by_group_params = dict()

# %%
# call the task


split_trajs_by_group = (
    split_groups.handle_errors(task_instance_id="split_trajs_by_group")
    .partial(
        df=rename_trajectory_cols,
        groupers=configure_grouping_strategy,
        **split_trajs_by_group_params,
    )
    .call()
)


# %% [markdown]
# ## Sort trajectories by speed bins

# %%
# parameters

sort_trajs_by_speed_params = dict(
    ascending=...,
)

# %%
# call the task


sort_trajs_by_speed = (
    sort_values.handle_errors(task_instance_id="sort_trajs_by_speed")
    .partial(column_name="speed_bins", na_position="last", **sort_trajs_by_speed_params)
    .mapvalues(argnames=["df"], argvalues=split_trajs_by_group)
)


# %% [markdown]
# ## Apply colormap to speedbins

# %%
# parameters

apply_speed_colormap_params = dict()

# %%
# call the task


apply_speed_colormap = (
    apply_color_map.handle_errors(task_instance_id="apply_speed_colormap")
    .partial(
        input_column_name="speed_bins",
        output_column_name="speed_bins_colormap",
        colormap=["#1a9850", "#91cf60", "#d9ef8b", "#fee08b", "#fc8d59", "#d73027"],
        **apply_speed_colormap_params,
    )
    .mapvalues(argnames=["df"], argvalues=sort_trajs_by_speed)
)


# %% [markdown]
# ## Format speed bins for legend

# %%
# parameters

format_speed_bin_labels_params = dict()

# %%
# call the task


format_speed_bin_labels = (
    map_values_with_unit.handle_errors(task_instance_id="format_speed_bin_labels")
    .partial(
        input_column_name="speed_bins",
        output_column_name="speed_bins_formatted",
        original_unit="km/h",
        new_unit="km/h",
        decimal_places=1,
        **format_speed_bin_labels_params,
    )
    .mapvalues(argnames=["df"], argvalues=apply_speed_colormap)
)


# %% [markdown]
# ## Format speed values for display

# %%
# parameters

format_speed_values_params = dict()

# %%
# call the task


format_speed_values = (
    map_values_with_unit.handle_errors(task_instance_id="format_speed_values")
    .partial(
        input_column_name="speed_kmhr",
        output_column_name="speed_kmhr",
        original_unit="km/h",
        new_unit="km/h",
        decimal_places=1,
        **format_speed_values_params,
    )
    .mapvalues(argnames=["df"], argvalues=format_speed_bin_labels)
)


# %% [markdown]
# ## Generata speedmap layers

# %%
# parameters

generate_speedmap_layers_params = dict(
    zoom=...,
)

# %%
# call the task


generate_speedmap_layers = (
    create_polyline_layer.handle_errors(task_instance_id="generate_speedmap_layers")
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        layer_style={"color_column": "speed_bins_colormap"},
        legend={
            "label_column": "speed_bins_formatted",
            "color_column": "speed_bins_colormap",
        },
        tooltip_columns=[
            "subject_name",
            "subject_sex",
            "subject_subtype",
            "speed_kmhr",
            "speed_bins",
            "dist_meters",
        ],
        **generate_speedmap_layers_params,
    )
    .mapvalues(argnames=["geodataframe"], argvalues=format_speed_values)
)


# %% [markdown]
# ## Zoom speed trajectories by view state

# %%
# parameters

zoom_traj_view_params = dict(
    zoom_value=...,
)

# %%
# call the task


zoom_traj_view = (
    create_view_state_from_gdf.handle_errors(task_instance_id="zoom_traj_view")
    .partial(pitch=0, bearing=0, **zoom_traj_view_params)
    .mapvalues(argnames=["gdf"], argvalues=format_speed_values)
)


# %% [markdown]
# ## Combine landdx and speedmap layers

# %%
# parameters

combine_landdx_speed_layers_params = dict()

# %%
# call the task


combine_landdx_speed_layers = (
    combine_map_layers.handle_errors(task_instance_id="combine_landdx_speed_layers")
    .partial(
        static_layers=create_styled_ldx_layers, **combine_landdx_speed_layers_params
    )
    .mapvalues(argnames=["grouped_layers"], argvalues=generate_speedmap_layers)
)


# %% [markdown]
# ## Zip speed layers and zoom values

# %%
# parameters

speedvalues_view_zip_params = dict()

# %%
# call the task


speedvalues_view_zip = (
    zip_grouped_by_key.handle_errors(task_instance_id="speedvalues_view_zip")
    .partial(
        left=combine_landdx_speed_layers,
        right=zoom_traj_view,
        **speedvalues_view_zip_params,
    )
    .call()
)


# %% [markdown]
# ## Draw Speedmap

# %%
# parameters

draw_speedmap_params = dict(
    widget_id=...,
)

# %%
# call the task


draw_speedmap = (
    draw_ecomap.handle_errors(task_instance_id="draw_speedmap")
    .partial(
        tile_layers=configure_base_maps,
        north_arrow_style={"placement": "top-left"},
        legend_style={"placement": "bottom-right", "title": "Speed Values(Km/h)"},
        static=False,
        title=None,
        max_zoom=20,
        **draw_speedmap_params,
    )
    .mapvalues(argnames=["geo_layers", "view_state"], argvalues=speedvalues_view_zip)
)


# %% [markdown]
# ## Persist Speed Ecomap HTML Paths

# %%
# parameters

persist_speed_ecomap_urls_params = dict(
    filename=...,
    filename_suffix=...,
)

# %%
# call the task


persist_speed_ecomap_urls = (
    persist_text.handle_errors(task_instance_id="persist_speed_ecomap_urls")
    .partial(
        root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        **persist_speed_ecomap_urls_params,
    )
    .mapvalues(argnames=["text"], argvalues=draw_speedmap)
)


# %% [markdown]
# ## Create Speed Ecomap Widgets

# %%
# parameters

create_speed_ecomap_widgets_params = dict()

# %%
# call the task


create_speed_ecomap_widgets = (
    create_map_widget_single_view.handle_errors(
        task_instance_id="create_speed_ecomap_widgets"
    )
    .skipif(
        conditions=[
            never,
        ],
        unpack_depth=1,
    )
    .partial(title="Speedmap", **create_speed_ecomap_widgets_params)
    .map(argnames=["view", "data"], argvalues=persist_speed_ecomap_urls)
)


# %% [markdown]
# ## Merge Speed Ecomap Widgets

# %%
# parameters

merge_speed_ecomap_widgets_params = dict()

# %%
# call the task


merge_speed_ecomap_widgets = (
    merge_widget_views.handle_errors(task_instance_id="merge_speed_ecomap_widgets")
    .partial(widgets=create_speed_ecomap_widgets, **merge_speed_ecomap_widgets_params)
    .call()
)


# %% [markdown]
# ## Generate home range ecomap

# %%
# parameters

generate_etd_params = dict(
    auto_scale_or_custom_cell_size=...,
    max_speed_factor=...,
    expansion_factor=...,
)

# %%
# call the task


generate_etd = (
    calculate_elliptical_time_density.handle_errors(task_instance_id="generate_etd")
    .partial(
        crs="ESRI:53042",
        percentiles=[50.0, 60.0, 70.0, 80.0, 90.0, 95.0, 99.9],
        nodata_value="nan",
        band_count=1,
        **generate_etd_params,
    )
    .mapvalues(argnames=["trajectory_gdf"], argvalues=split_trajs_by_group)
)


# %% [markdown]
# ## Determine Seasonal Windows

# %%
# parameters

determine_seasonal_windows_params = dict()

# %%
# call the task


determine_seasonal_windows = (
    determine_season_windows.handle_errors(
        task_instance_id="determine_seasonal_windows"
    )
    .partial(
        client=gee_client,
        time_range=define_time_range,
        **determine_seasonal_windows_params,
    )
    .mapvalues(argnames=["roi"], argvalues=generate_etd)
)


# %% [markdown]
# ## Zip ETD and grouped trajs

# %%
# parameters

zip_etd_and_grouped_trajs_params = dict()

# %%
# call the task


zip_etd_and_grouped_trajs = (
    zip_grouped_by_key.handle_errors(task_instance_id="zip_etd_and_grouped_trajs")
    .partial(
        left=determine_seasonal_windows,
        right=split_trajs_by_group,
        **zip_etd_and_grouped_trajs_params,
    )
    .call()
)


# %% [markdown]
# ## Generate seasonal labels

# %%
# parameters

add_season_labels_params = dict()

# %%
# call the task


add_season_labels = (
    create_seasonal_labels.handle_errors(task_instance_id="add_season_labels")
    .partial(**add_season_labels_params)
    .mapvalues(
        argnames=["total_percentiles", "traj"], argvalues=zip_etd_and_grouped_trajs
    )
)


# %% [markdown]
# ## Generate MCP gdf

# %%
# parameters

calculate_mcp_params = dict()

# %%
# call the task


calculate_mcp = (
    generate_mcp_gdf.handle_errors(task_instance_id="calculate_mcp")
    .partial(planar_crs="ESRI:53042", **calculate_mcp_params)
    .mapvalues(argnames=["gdf"], argvalues=split_trajs_by_group)
)


# %% [markdown]
# ## Apply colormap to etd percentiles

# %%
# parameters

apply_etd_percentile_colormap_params = dict()

# %%
# call the task


apply_etd_percentile_colormap = (
    apply_color_map.handle_errors(task_instance_id="apply_etd_percentile_colormap")
    .partial(
        input_column_name="percentile",
        colormap="RdYlGn",
        output_column_name="percentile_colormap",
        **apply_etd_percentile_colormap_params,
    )
    .mapvalues(argnames=["df"], argvalues=generate_etd)
)


# %% [markdown]
# ## Create hr etd layers

# %%
# parameters

generate_etd_ecomap_layers_params = dict(
    zoom=...,
)

# %%
# call the task


generate_etd_ecomap_layers = (
    create_polygon_layer.handle_errors(task_instance_id="generate_etd_ecomap_layers")
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        layer_style={"fill_color_column": "percentile_colormap", "opacity": 0.55},
        legend={"label_column": "percentile", "color_column": "percentile_colormap"},
        tooltip_columns=["percentile"],
        **generate_etd_ecomap_layers_params,
    )
    .mapvalues(argnames=["geodataframe"], argvalues=apply_etd_percentile_colormap)
)


# %% [markdown]
# ## Zoom hr movement by view state

# %%
# parameters

zoom_hr_view_params = dict(
    zoom_value=...,
)

# %%
# call the task


zoom_hr_view = (
    create_view_state_from_gdf.handle_errors(task_instance_id="zoom_hr_view")
    .partial(pitch=0, bearing=0, **zoom_hr_view_params)
    .mapvalues(argnames=["gdf"], argvalues=apply_etd_percentile_colormap)
)


# %% [markdown]
# ## Combine ldx and hr layers

# %%
# parameters

combine_landdx_hr_ecomap_layers_params = dict()

# %%
# call the task


combine_landdx_hr_ecomap_layers = (
    combine_map_layers.handle_errors(task_instance_id="combine_landdx_hr_ecomap_layers")
    .partial(
        static_layers=create_styled_ldx_layers, **combine_landdx_hr_ecomap_layers_params
    )
    .mapvalues(argnames=["grouped_layers"], argvalues=generate_etd_ecomap_layers)
)


# %% [markdown]
# ## Zip hr layers and zoom values

# %%
# parameters

hr_view_zip_params = dict()

# %%
# call the task


hr_view_zip = (
    zip_grouped_by_key.handle_errors(task_instance_id="hr_view_zip")
    .partial(
        left=combine_landdx_hr_ecomap_layers, right=zoom_hr_view, **hr_view_zip_params
    )
    .call()
)


# %% [markdown]
# ## Draw Home Range Ecomaps by Group

# %%
# parameters

draw_hr_ecomaps_params = dict(
    widget_id=...,
)

# %%
# call the task


draw_hr_ecomaps = (
    draw_ecomap.handle_errors(task_instance_id="draw_hr_ecomaps")
    .partial(
        tile_layers=configure_base_maps,
        north_arrow_style={"placement": "top-left"},
        legend_style={"placement": "bottom-right", "title": "Home Range Metrics"},
        static=False,
        title=None,
        max_zoom=20,
        **draw_hr_ecomaps_params,
    )
    .mapvalues(argnames=["geo_layers", "view_state"], argvalues=hr_view_zip)
)


# %% [markdown]
# ## Persist hr ecomap html urls

# %%
# parameters

persist_hr_ecomap_urls_params = dict(
    filename=...,
    filename_suffix=...,
)

# %%
# call the task


persist_hr_ecomap_urls = (
    persist_text.handle_errors(task_instance_id="persist_hr_ecomap_urls")
    .partial(
        root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        **persist_hr_ecomap_urls_params,
    )
    .mapvalues(argnames=["text"], argvalues=draw_hr_ecomaps)
)


# %% [markdown]
# ## Create hr ecomap widgets

# %%
# parameters

create_hr_ecomap_widgets_params = dict()

# %%
# call the task


create_hr_ecomap_widgets = (
    create_map_widget_single_view.handle_errors(
        task_instance_id="create_hr_ecomap_widgets"
    )
    .skipif(
        conditions=[
            never,
        ],
        unpack_depth=1,
    )
    .partial(title="Home Range Ecomap", **create_hr_ecomap_widgets_params)
    .map(argnames=["view", "data"], argvalues=persist_hr_ecomap_urls)
)


# %% [markdown]
# ## Merge hr ecomap widgets

# %%
# parameters

merge_hr_ecomap_widgets_params = dict()

# %%
# call the task


merge_hr_ecomap_widgets = (
    merge_widget_views.handle_errors(task_instance_id="merge_hr_ecomap_widgets")
    .partial(widgets=create_hr_ecomap_widgets, **merge_hr_ecomap_widgets_params)
    .call()
)


# %% [markdown]
# ## Calculate seasonal home range

# %%
# parameters

seasonal_home_range_params = dict(
    percentiles=...,
    auto_scale_or_custom_cell_size=...,
)

# %%
# call the task


seasonal_home_range = (
    calculate_seasonal_home_range.handle_errors(task_instance_id="seasonal_home_range")
    .skipif(
        conditions=[
            any_is_empty_df,
            all_geometry_are_none,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(groupby_cols=["subject_name", "season"], **seasonal_home_range_params)
    .mapvalues(argnames=["gdf"], argvalues=add_season_labels)
)


# %% [markdown]
# ## time density colormap

# %%
# parameters

season_colormap_params = dict()

# %%
# call the task


season_colormap = (
    apply_color_map.handle_errors(task_instance_id="season_colormap")
    .partial(
        input_column_name="season",
        output_column_name="season_colormap",
        colormap=["#f57c00", "#4cf3f7"],
        **season_colormap_params,
    )
    .mapvalues(argnames=["df"], argvalues=seasonal_home_range)
)


# %% [markdown]
# ## Create season layers

# %%
# parameters

season_etd_map_layer_params = dict(
    zoom=...,
)

# %%
# call the task


season_etd_map_layer = (
    create_polygon_layer.handle_errors(task_instance_id="season_etd_map_layer")
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        layer_style={"fill_color_column": "season_colormap", "opacity": 0.65},
        legend={"label_column": "season", "color_column": "season_colormap"},
        tooltip_columns=["percentile"],
        **season_etd_map_layer_params,
    )
    .mapvalues(argnames=["geodataframe"], argvalues=season_colormap)
)


# %% [markdown]
# ## Create MCP Polygon Layer

# %%
# parameters

generate_mcp_layers_params = dict(
    zoom=...,
)

# %%
# call the task


generate_mcp_layers = (
    create_polygon_layer.handle_errors(task_instance_id="generate_mcp_layers")
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        layer_style={
            "get_fill_color": "#FFFFFF00",
            "get_line_color": "#dc143c",
            "opacity": 0.55,
            "stroked": True,
        },
        legend={"labels": ["mcp"], "colors": ["#dc143c"]},
        tooltip_columns=["area_km2"],
        **generate_mcp_layers_params,
    )
    .mapvalues(argnames=["geodataframe"], argvalues=calculate_mcp)
)


# %% [markdown]
# ## zip mcp and season hr

# %%
# parameters

zip_season_mcp_hr_params = dict()

# %%
# call the task


zip_season_mcp_hr = (
    zip_grouped_by_key.handle_errors(task_instance_id="zip_season_mcp_hr")
    .partial(
        left=generate_mcp_layers, right=season_etd_map_layer, **zip_season_mcp_hr_params
    )
    .call()
)


# %% [markdown]
# ## Zoom seasons by view state

# %%
# parameters

zoom_season_view_params = dict(
    zoom_value=...,
)

# %%
# call the task


zoom_season_view = (
    create_view_state_from_gdf.handle_errors(task_instance_id="zoom_season_view")
    .partial(pitch=0, bearing=0, **zoom_season_view_params)
    .mapvalues(argnames=["gdf"], argvalues=season_colormap)
)


# %% [markdown]
# ## Combine map layers

# %%
# parameters

comb_season_map_layers_params = dict()

# %%
# call the task


comb_season_map_layers = (
    combine_map_layers.handle_errors(task_instance_id="comb_season_map_layers")
    .partial(static_layers=create_styled_ldx_layers, **comb_season_map_layers_params)
    .mapvalues(argnames=["grouped_layers"], argvalues=zip_season_mcp_hr)
)


# %% [markdown]
# ## Zip seasons layers and zoom values

# %%
# parameters

seasons_view_zip_params = dict()

# %%
# call the task


seasons_view_zip = (
    zip_grouped_by_key.handle_errors(task_instance_id="seasons_view_zip")
    .partial(
        left=comb_season_map_layers, right=zoom_season_view, **seasons_view_zip_params
    )
    .call()
)


# %% [markdown]
# ## Draw Season Ecomap

# %%
# parameters

seasonal_ecomap_params = dict(
    widget_id=...,
)

# %%
# call the task


seasonal_ecomap = (
    draw_ecomap.handle_errors(task_instance_id="seasonal_ecomap")
    .partial(
        tile_layers=configure_base_maps,
        north_arrow_style={"placement": "top-left"},
        legend_style={"placement": "bottom-right", "title": "Seasons"},
        static=False,
        title=None,
        max_zoom=20,
        **seasonal_ecomap_params,
    )
    .mapvalues(argnames=["geo_layers", "view_state"], argvalues=seasons_view_zip)
)


# %% [markdown]
# ## Perist HR ecomap as text

# %%
# parameters

season_etd_ecomap_html_url_params = dict(
    filename=...,
    filename_suffix=...,
)

# %%
# call the task


season_etd_ecomap_html_url = (
    persist_text.handle_errors(task_instance_id="season_etd_ecomap_html_url")
    .partial(
        root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        **season_etd_ecomap_html_url_params,
    )
    .mapvalues(argnames=["text"], argvalues=seasonal_ecomap)
)


# %% [markdown]
# ## Create Map Widgets for seasons

# %%
# parameters

season_etd_widgets_single_view_params = dict()

# %%
# call the task


season_etd_widgets_single_view = (
    create_map_widget_single_view.handle_errors(
        task_instance_id="season_etd_widgets_single_view"
    )
    .skipif(
        conditions=[
            never,
        ],
        unpack_depth=1,
    )
    .partial(
        title="Subject Group Home Range Map (Seasons)",
        **season_etd_widgets_single_view_params,
    )
    .map(argnames=["view", "data"], argvalues=season_etd_ecomap_html_url)
)


# %% [markdown]
# ## Merge Ecomap Widgets

# %%
# parameters

season_grouped_map_widget_params = dict()

# %%
# call the task


season_grouped_map_widget = (
    merge_widget_views.handle_errors(task_instance_id="season_grouped_map_widget")
    .partial(widgets=season_etd_widgets_single_view, **season_grouped_map_widget_params)
    .call()
)


# %% [markdown]
# ## Persist seasonal windows as csv

# %%
# parameters

persist_subject_season_wins_params = dict(
    filename=...,
    filetype=...,
)

# %%
# call the task


persist_subject_season_wins = (
    persist_df.handle_errors(task_instance_id="persist_subject_season_wins")
    .partial(
        root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        **persist_subject_season_wins_params,
    )
    .mapvalues(argnames=["df"], argvalues=determine_seasonal_windows)
)


# %% [markdown]
# ## Zip gdf and seasonal windows

# %%
# parameters

zip_nsd_values_params = dict()

# %%
# call the task


zip_nsd_values = (
    zip_grouped_by_key.handle_errors(task_instance_id="zip_nsd_values")
    .partial(
        left=split_relocs_by_group,
        right=persist_subject_season_wins,
        **zip_nsd_values_params,
    )
    .call()
)


# %% [markdown]
# ## Generate seasonal nsd plot

# %%
# parameters

generate_nsd_plot_params = dict(
    widget_id=...,
)

# %%
# call the task


generate_nsd_plot = (
    generate_seasonal_nsd_plot.handle_errors(task_instance_id="generate_nsd_plot")
    .partial(**generate_nsd_plot_params)
    .mapvalues(argnames=["gdf", "seasons_df"], argvalues=zip_nsd_values)
)


# %% [markdown]
# ## Persist seasonal nsd plots

# %%
# parameters

persist_nsd_html_urls_params = dict(
    filename=...,
    filename_suffix=...,
)

# %%
# call the task


persist_nsd_html_urls = (
    persist_text.handle_errors(task_instance_id="persist_nsd_html_urls")
    .partial(
        root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        **persist_nsd_html_urls_params,
    )
    .mapvalues(argnames=["text"], argvalues=generate_nsd_plot)
)


# %% [markdown]
# ## Create widget for nsd plot

# %%
# parameters

nsd_plot_widgets_single_view_params = dict()

# %%
# call the task


nsd_plot_widgets_single_view = (
    create_map_widget_single_view.handle_errors(
        task_instance_id="nsd_plot_widgets_single_view"
    )
    .skipif(
        conditions=[
            never,
        ],
        unpack_depth=1,
    )
    .partial(
        title="Net Square Displacement (NSD)", **nsd_plot_widgets_single_view_params
    )
    .map(argnames=["view", "data"], argvalues=persist_nsd_html_urls)
)


# %% [markdown]
# ## Merge nsd plot widgets

# %%
# parameters

grouped_nsd_plot_widget_params = dict()

# %%
# call the task


grouped_nsd_plot_widget = (
    merge_widget_views.handle_errors(task_instance_id="grouped_nsd_plot_widget")
    .partial(widgets=nsd_plot_widgets_single_view, **grouped_nsd_plot_widget_params)
    .call()
)


# %% [markdown]
# ## zip gdf and seasonal windows for speed_plot

# %%
# parameters

zip_speed_values_params = dict()

# %%
# call the task


zip_speed_values = (
    zip_grouped_by_key.handle_errors(task_instance_id="zip_speed_values")
    .partial(
        left=split_relocs_by_group,
        right=persist_subject_season_wins,
        **zip_speed_values_params,
    )
    .call()
)


# %% [markdown]
# ## Generate seasonal speed plot

# %%
# parameters

generate_speed_plot_params = dict(
    widget_id=...,
)

# %%
# call the task


generate_speed_plot = (
    generate_seasonal_speed_plot.handle_errors(task_instance_id="generate_speed_plot")
    .partial(**generate_speed_plot_params)
    .mapvalues(argnames=["gdf", "seasons_df"], argvalues=zip_speed_values)
)


# %% [markdown]
# ## Persist seasonal speed plots

# %%
# parameters

persist_speed_html_urls_params = dict(
    filename=...,
    filename_suffix=...,
)

# %%
# call the task


persist_speed_html_urls = (
    persist_text.handle_errors(task_instance_id="persist_speed_html_urls")
    .partial(
        root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        **persist_speed_html_urls_params,
    )
    .mapvalues(argnames=["text"], argvalues=generate_speed_plot)
)


# %% [markdown]
# ## Create widget for speed plot

# %%
# parameters

speed_plot_widgets_single_view_params = dict()

# %%
# call the task


speed_plot_widgets_single_view = (
    create_map_widget_single_view.handle_errors(
        task_instance_id="speed_plot_widgets_single_view"
    )
    .skipif(
        conditions=[
            never,
        ],
        unpack_depth=1,
    )
    .partial(title="Speed", **speed_plot_widgets_single_view_params)
    .map(argnames=["view", "data"], argvalues=persist_speed_html_urls)
)


# %% [markdown]
# ## Merge speed plot widgets

# %%
# parameters

grouped_speed_plot_widget_params = dict()

# %%
# call the task


grouped_speed_plot_widget = (
    merge_widget_views.handle_errors(task_instance_id="grouped_speed_plot_widget")
    .partial(widgets=speed_plot_widgets_single_view, **grouped_speed_plot_widget_params)
    .call()
)


# %% [markdown]
# ## Zip gdf for collared subject plot

# %%
# parameters

zip_collared_subject_values_params = dict()

# %%
# call the task


zip_collared_subject_values = (
    zip_grouped_by_key.handle_errors(task_instance_id="zip_collared_subject_values")
    .partial(
        left=split_relocs_by_group,
        right=persist_subject_season_wins,
        **zip_collared_subject_values_params,
    )
    .call()
)


# %% [markdown]
# ## Generate collared subject plot

# %%
# parameters

generate_colared_subject_plot_params = dict(
    widget_id=...,
)

# %%
# call the task


generate_colared_subject_plot = (
    generate_collared_seasonal_plot.handle_errors(
        task_instance_id="generate_colared_subject_plot"
    )
    .partial(
        events_gdf=rename_event_cols,
        filter_col="subject_name",
        **generate_colared_subject_plot_params,
    )
    .mapvalues(
        argnames=["relocations_gdf", "seasons_df"],
        argvalues=zip_collared_subject_values,
    )
)


# %% [markdown]
# ## Persist collared subject plots

# %%
# parameters

persist_collared_subject_plots_params = dict(
    filename=...,
    filename_suffix=...,
)

# %%
# call the task


persist_collared_subject_plots = (
    persist_text.handle_errors(task_instance_id="persist_collared_subject_plots")
    .partial(
        root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        **persist_collared_subject_plots_params,
    )
    .mapvalues(argnames=["text"], argvalues=generate_colared_subject_plot)
)


# %% [markdown]
# ## Create widget for collared subject plot

# %%
# parameters

collared_widget_view_params = dict()

# %%
# call the task


collared_widget_view = (
    create_map_widget_single_view.handle_errors(task_instance_id="collared_widget_view")
    .skipif(
        conditions=[
            never,
        ],
        unpack_depth=1,
    )
    .partial(title="Collared Subject Plot", **collared_widget_view_params)
    .map(argnames=["view", "data"], argvalues=persist_collared_subject_plots)
)


# %% [markdown]
# ## Merge collared plot widgets

# %%
# parameters

grouped_collared_widget_params = dict()

# %%
# call the task


grouped_collared_widget = (
    merge_widget_views.handle_errors(task_instance_id="grouped_collared_widget")
    .partial(widgets=collared_widget_view, **grouped_collared_widget_params)
    .call()
)


# %% [markdown]
# ## Zip gdf for mcp asymptote plot

# %%
# parameters

zip_mcp_asymp_values_params = dict()

# %%
# call the task


zip_mcp_asymp_values = (
    zip_grouped_by_key.handle_errors(task_instance_id="zip_mcp_asymp_values")
    .partial(
        left=split_relocs_by_group,
        right=persist_subject_season_wins,
        **zip_mcp_asymp_values_params,
    )
    .call()
)


# %% [markdown]
# ## Generate mcp asymptote plot

# %%
# parameters

generate_mcp_asymp_plot_params = dict(
    widget_id=...,
)

# %%
# call the task


generate_mcp_asymp_plot = (
    generate_seasonal_mcp_asymptote_plot.handle_errors(
        task_instance_id="generate_mcp_asymp_plot"
    )
    .partial(**generate_mcp_asymp_plot_params)
    .mapvalues(argnames=["gdf", "seasons_df"], argvalues=zip_mcp_asymp_values)
)


# %% [markdown]
# ## Persist mcp asymptote plots

# %%
# parameters

persist_mcp_html_urls_params = dict(
    filename=...,
    filename_suffix=...,
)

# %%
# call the task


persist_mcp_html_urls = (
    persist_text.handle_errors(task_instance_id="persist_mcp_html_urls")
    .partial(
        root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        **persist_mcp_html_urls_params,
    )
    .mapvalues(argnames=["text"], argvalues=generate_mcp_asymp_plot)
)


# %% [markdown]
# ## Create widget for mcp plot

# %%
# parameters

mcp_plot_widgets_single_view_params = dict()

# %%
# call the task


mcp_plot_widgets_single_view = (
    create_map_widget_single_view.handle_errors(
        task_instance_id="mcp_plot_widgets_single_view"
    )
    .skipif(
        conditions=[
            never,
        ],
        unpack_depth=1,
    )
    .partial(title="MCP Asymptote", **mcp_plot_widgets_single_view_params)
    .map(argnames=["view", "data"], argvalues=persist_mcp_html_urls)
)


# %% [markdown]
# ## Merge mcp plot widgets

# %%
# parameters

grouped_mcp_plot_widget_params = dict()

# %%
# call the task


grouped_mcp_plot_widget = (
    merge_widget_views.handle_errors(task_instance_id="grouped_mcp_plot_widget")
    .partial(widgets=mcp_plot_widgets_single_view, **grouped_mcp_plot_widget_params)
    .call()
)


# %% [markdown]
# ## Zip traj gdf and etd gdf

# %%
# parameters

zip_traj_etd_gdf_params = dict()

# %%
# call the task


zip_traj_etd_gdf = (
    zip_grouped_by_key.handle_errors(task_instance_id="zip_traj_etd_gdf")
    .partial(left=generate_etd, right=split_trajs_by_group, **zip_traj_etd_gdf_params)
    .call()
)


# %% [markdown]
# ## Generate subject stats

# %%
# parameters

generate_subject_stats_params = dict(
    return_dataframe=...,
)

# %%
# call the task


generate_subject_stats = (
    get_subject_stats.handle_errors(task_instance_id="generate_subject_stats")
    .partial(
        output_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        subject_df=compute_subject_maturity,
        groupby_col="subject_name",
        **generate_subject_stats_params,
    )
    .mapvalues(argnames=["etd_df", "traj_gdf"], argvalues=zip_traj_etd_gdf)
)


# %% [markdown]
# ## Persist subject stats as csv

# %%
# parameters

persist_subject_stats_params = dict(
    filename=...,
    filetype=...,
)

# %%
# call the task


persist_subject_stats = (
    persist_df.handle_errors(task_instance_id="persist_subject_stats")
    .partial(
        root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        **persist_subject_stats_params,
    )
    .mapvalues(argnames=["df"], argvalues=generate_subject_stats)
)


# %% [markdown]
# ## Load Unfiltered Landdx

# %%
# parameters

load_unfiltered_ldx_params = dict(
    aoi=...,
)

# %%
# call the task


load_unfiltered_ldx = (
    load_landdx_aoi.handle_errors(task_instance_id="load_unfiltered_ldx")
    .partial(map_path=retrieve_ldx_db, **load_unfiltered_ldx_params)
    .call()
)


# %% [markdown]
# ## Zip etd gdf and subject df

# %%
# parameters

zip_etd_subjects_params = dict()

# %%
# call the task


zip_etd_subjects = (
    zip_grouped_by_key.handle_errors(task_instance_id="zip_etd_subjects")
    .partial(left=generate_etd, right=split_subject_by_group, **zip_etd_subjects_params)
    .call()
)


# %% [markdown]
# ## Build regional lookup from landDx db

# %%
# parameters

build_region_lookup_params = dict(
    categories=...,
    static_ids=...,
)

# %%
# call the task


build_region_lookup = (
    build_template_region_lookup.handle_errors(task_instance_id="build_region_lookup")
    .partial(gdf=load_unfiltered_ldx, **build_region_lookup_params)
    .call()
)


# %% [markdown]
# ## Compute template regions

# %%
# parameters

comp_template_regions_params = dict()

# %%
# call the task


comp_template_regions = (
    compute_template_regions.handle_errors(task_instance_id="comp_template_regions")
    .partial(
        geodataframe=load_unfiltered_ldx,
        template_lookup=build_region_lookup,
        crs="ESRI:53042",
        **comp_template_regions_params,
    )
    .call()
)


# %% [markdown]
# ## Calculate subject occupancy

# %%
# parameters

process_subject_occupancy_params = dict()

# %%
# call the task


process_subject_occupancy = (
    compute_subject_occupancy.handle_errors(
        task_instance_id="process_subject_occupancy"
    )
    .partial(
        crs="ESRI:53042",
        regions_gdf=comp_template_regions,
        output_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        **process_subject_occupancy_params,
    )
    .mapvalues(argnames=["etd_gdf", "subjects_df"], argvalues=zip_etd_subjects)
)


# %% [markdown]
# ## persist subject occupancy as csv

# %%
# parameters

persist_subject_occupancy_params = dict(
    filename=...,
    filetype=...,
)

# %%
# call the task


persist_subject_occupancy = (
    persist_df.handle_errors(task_instance_id="persist_subject_occupancy")
    .partial(
        root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        **persist_subject_occupancy_params,
    )
    .mapvalues(argnames=["df"], argvalues=process_subject_occupancy)
)


# %% [markdown]
# ## Convert range ecomap html to png

# %%
# parameters

convt_range_html_png_params = dict()

# %%
# call the task


convt_range_html_png = (
    html_to_png.handle_errors(task_instance_id="convt_range_html_png")
    .partial(
        output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        config={"wait_for_timeout": 20, "width": 765, "height": 525},
        **convt_range_html_png_params,
    )
    .mapvalues(argnames=["html_path"], argvalues=persist_hr_ecomap_urls)
)


# %% [markdown]
# ## Convert speedmap html to png

# %%
# parameters

convt_speedmap_html_png_params = dict()

# %%
# call the task


convt_speedmap_html_png = (
    html_to_png.handle_errors(task_instance_id="convt_speedmap_html_png")
    .partial(
        output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        config={"wait_for_timeout": 20, "width": 765, "height": 525},
        **convt_speedmap_html_png_params,
    )
    .mapvalues(argnames=["html_path"], argvalues=persist_speed_ecomap_urls)
)


# %% [markdown]
# ## Convert seasons ecomap html to png

# %%
# parameters

convt_seasons_html_png_params = dict()

# %%
# call the task


convt_seasons_html_png = (
    html_to_png.handle_errors(task_instance_id="convt_seasons_html_png")
    .partial(
        output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        config={"wait_for_timeout": 20, "width": 602, "height": 855},
        **convt_seasons_html_png_params,
    )
    .mapvalues(argnames=["html_path"], argvalues=season_etd_ecomap_html_url)
)


# %% [markdown]
# ## Convert nsd plot html to png

# %%
# parameters

convt_nsdp_html_png_params = dict()

# %%
# call the task


convt_nsdp_html_png = (
    html_to_png.handle_errors(task_instance_id="convt_nsdp_html_png")
    .partial(
        output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        config={"wait_for_timeout": 10, "width": 2238, "height": 450},
        **convt_nsdp_html_png_params,
    )
    .mapvalues(argnames=["html_path"], argvalues=persist_nsd_html_urls)
)


# %% [markdown]
# ## Convert mcp plot html to png

# %%
# parameters

convt_mcp_html_png_params = dict()

# %%
# call the task


convt_mcp_html_png = (
    html_to_png.handle_errors(task_instance_id="convt_mcp_html_png")
    .partial(
        output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        config={"wait_for_timeout": 10, "width": 2238, "height": 450},
        **convt_mcp_html_png_params,
    )
    .mapvalues(argnames=["html_path"], argvalues=persist_mcp_html_urls)
)


# %% [markdown]
# ## Convert speed plot html to png

# %%
# parameters

convt_speedp_html_png_params = dict()

# %%
# call the task


convt_speedp_html_png = (
    html_to_png.handle_errors(task_instance_id="convt_speedp_html_png")
    .partial(
        output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        config={"wait_for_timeout": 10, "width": 2238, "height": 450},
        **convt_speedp_html_png_params,
    )
    .mapvalues(argnames=["html_path"], argvalues=persist_speed_html_urls)
)


# %% [markdown]
# ## Convert collared events plot html to png

# %%
# parameters

convt_colev_html_png_params = dict()

# %%
# call the task


convt_colev_html_png = (
    html_to_png.handle_errors(task_instance_id="convt_colev_html_png")
    .partial(
        output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        config={"wait_for_timeout": 10, "width": 2238, "height": 450},
        **convt_colev_html_png_params,
    )
    .mapvalues(argnames=["html_path"], argvalues=persist_collared_subject_plots)
)


# %% [markdown]
# ## Zip movement and range ecomaps

# %%
# parameters

zip_movement_range_maps_params = dict()

# %%
# call the task


zip_movement_range_maps = (
    zip_grouped_by_key.handle_errors(task_instance_id="zip_movement_range_maps")
    .partial(
        left=convt_speedmap_html_png,
        right=convt_range_html_png,
        **zip_movement_range_maps_params,
    )
    .call()
)


# %% [markdown]
# ## zip movement range and overview map

# %%
# parameters

zip_range_mov_overview_params = dict()

# %%
# call the task


zip_range_mov_overview = (
    zip_grouped_by_key.handle_errors(task_instance_id="zip_range_mov_overview")
    .partial(
        left=zip_movement_range_maps,
        right=convt_seasons_html_png,
        **zip_range_mov_overview_params,
    )
    .call()
)


# %% [markdown]
# ## zip movement ,range ,overview maps and subject photo

# %%
# parameters

zip_range_subject_photo_params = dict()

# %%
# call the task


zip_range_subject_photo = (
    zip_grouped_by_key.handle_errors(task_instance_id="zip_range_subject_photo")
    .partial(
        left=zip_range_mov_overview,
        right=download_profile_pic,
        **zip_range_subject_photo_params,
    )
    .call()
)


# %% [markdown]
# ## zip existing list with collared plot

# %%
# parameters

zip_items_w_collared_params = dict()

# %%
# call the task


zip_items_w_collared = (
    zip_grouped_by_key.handle_errors(task_instance_id="zip_items_w_collared")
    .partial(
        left=zip_range_subject_photo,
        right=convt_colev_html_png,
        **zip_items_w_collared_params,
    )
    .call()
)


# %% [markdown]
# ## zip existing with nsd plot

# %%
# parameters

zip_with_nsd_params = dict()

# %%
# call the task


zip_with_nsd = (
    zip_grouped_by_key.handle_errors(task_instance_id="zip_with_nsd")
    .partial(
        left=zip_items_w_collared, right=convt_nsdp_html_png, **zip_with_nsd_params
    )
    .call()
)


# %% [markdown]
# ## zip existing with speed plot

# %%
# parameters

zip_with_speed_params = dict()

# %%
# call the task


zip_with_speed = (
    zip_grouped_by_key.handle_errors(task_instance_id="zip_with_speed")
    .partial(left=zip_with_nsd, right=convt_speedp_html_png, **zip_with_speed_params)
    .call()
)


# %% [markdown]
# ## zip existing with mcp plot

# %%
# parameters

zip_with_mcp_params = dict()

# %%
# call the task


zip_with_mcp = (
    zip_grouped_by_key.handle_errors(task_instance_id="zip_with_mcp")
    .partial(left=zip_with_speed, right=convt_mcp_html_png, **zip_with_mcp_params)
    .call()
)


# %% [markdown]
# ## zip existing with subject info

# %%
# parameters

zip_subject_info_params = dict()

# %%
# call the task


zip_subject_info = (
    zip_grouped_by_key.handle_errors(task_instance_id="zip_subject_info")
    .partial(left=zip_with_mcp, right=persist_subject_info, **zip_subject_info_params)
    .call()
)


# %% [markdown]
# ## zip existing with subject stats

# %%
# parameters

zip_subject_stats_params = dict()

# %%
# call the task


zip_subject_stats = (
    zip_grouped_by_key.handle_errors(task_instance_id="zip_subject_stats")
    .partial(
        left=zip_subject_info, right=persist_subject_stats, **zip_subject_stats_params
    )
    .call()
)


# %% [markdown]
# ## zip existing with occupancy info

# %%
# parameters

zip_occupancy_info_params = dict()

# %%
# call the task


zip_occupancy_info = (
    zip_grouped_by_key.handle_errors(task_instance_id="zip_occupancy_info")
    .partial(
        left=zip_subject_stats,
        right=persist_subject_occupancy,
        **zip_occupancy_info_params,
    )
    .call()
)


# %% [markdown]
# ## Flatten zipped collared report context inputs

# %%
# parameters

flatten_collared_context_params = dict()

# %%
# call the task


flatten_collared_context = (
    flatten_tuple.handle_errors(task_instance_id="flatten_collared_context")
    .partial(**flatten_collared_context_params)
    .mapvalues(argnames=["nested"], argvalues=zip_occupancy_info)
)


# %% [markdown]
# ## View flatten info outputs

# %%
# parameters

view_flatten_data_params = dict()

# %%
# call the task


view_flatten_data = (
    print_output.handle_errors(task_instance_id="view_flatten_data")
    .partial(value=flatten_collared_context, **view_flatten_data_params)
    .call()
)


# %% [markdown]
# ## Generate report context

# %%
# parameters

generate_report_context_params = dict()

# %%
# call the task


generate_report_context = (
    report_context.handle_errors(task_instance_id="generate_report_context")
    .partial(
        subjects_df=compute_subject_maturity,
        templates=[
            download_template_one,
            download_template_two,
            download_template_three,
        ],
        input_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        logo=download_logo,
        **generate_report_context_params,
    )
    .mapvalues(
        argnames=[
            "movement_ecomap",
            "range_ecomap",
            "overview_map",
            "subject_photo",
            "collar_event_timeline_plot",
            "nsd_plot",
            "speed_plot",
            "mcp_plot",
            "subject_info",
            "subject_stats",
            "occupancy_info",
        ],
        argvalues=flatten_collared_context,
    )
)


# %% [markdown]
# ## Render html to pdf

# %%
# parameters

render_html_pdf_params = dict()

# %%
# call the task


render_html_pdf = (
    render_html_to_pdf.handle_errors(task_instance_id="render_html_pdf")
    .partial(
        pdf_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"], **render_html_pdf_params
    )
    .mapvalues(argnames=["html_path"], argvalues=generate_report_context)
)


# %% [markdown]
# ## Merge pdfs

# %%
# parameters

merge_subject_pdf_params = dict()

# %%
# call the task


merge_subject_pdf = (
    merge_pdfs.handle_errors(task_instance_id="merge_subject_pdf")
    .partial(
        subject_df=compute_subject_maturity,
        output_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        filename="Collared Elephants Report",
        pdf_path_items=render_html_pdf,
        **merge_subject_pdf_params,
    )
    .call()
)


# %% [markdown]
# ## Collared Elephant Report Dashboard

# %%
# parameters

collared_report_template_params = dict()

# %%
# call the task


collared_report_template = (
    gather_dashboard.handle_errors(task_instance_id="collared_report_template")
    .partial(
        details=workflow_details,
        widgets=[
            merge_speed_ecomap_widgets,
            merge_hr_ecomap_widgets,
            season_grouped_map_widget,
            grouped_nsd_plot_widget,
            grouped_speed_plot_widget,
            grouped_collared_widget,
            grouped_mcp_plot_widget,
        ],
        time_range=define_time_range,
        groupers=configure_grouping_strategy,
        **collared_report_template_params,
    )
    .call()
)
